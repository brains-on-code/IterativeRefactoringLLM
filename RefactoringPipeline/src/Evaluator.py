import argparse
import os
from pathlib import Path
import shutil
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
import pickle
import scipy.stats as stats
from statsmodels.formula.api import mixedlm
from itertools import combinations
from statsmodels.stats.multitest import multipletests
import warnings
from statsmodels.tools.sm_exceptions import ConvergenceWarning

import config.ast_config
import config.constants
import config.fields
import config.utils
import src.VisualizeAnalysis as va
from config.utils import ask_user

# Setup logging
logging, handler = config.utils.setup_logger("Evaluator")


class Evaluator:

    def __init__(self, input_dir):
        logging.info("Initializing Evaluator...")

        self.results_dir = input_dir
        self.analysis_dir = os.path.join(self.results_dir, "analysis")

        os.makedirs(self.analysis_dir, exist_ok=True)

        logging.info(f"Input dir: {self.results_dir}, Analysis dir: {self.analysis_dir}")

    def load_absolute_values_to_df(self):
        """
        Function to read in the Absolute_Values.csv in the directory
        and process it into a dataframe.

        :return: The dataframe.
        """

        # Choose CSV file that contain the absolute values
        filename = os.path.join(self.results_dir, f"{config.constants.ABSOLUTE_VALUES}.csv")
        if not os.path.exists(filename):
            raise FileNotFoundError(
                f"No CSV-file '{filename}' found.")


        logging.info("Loading Absolute Values into dataframe...")
        # Import CSV file of the absolute values
        df = pd.read_csv(filename)

        required_cols = config.fields.ABSOLUTE_VALUES_REPORT_HEADER

        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"(Absolute Values Report) Column '{col}' not in DataFrame.")

        # Important step:
        # Don't keep the "no prompt" entries, but replace them such that there is an entry for
        # each real prompt-line (because all prompts start with code that has not been generated
        # by a prompt). This way we don't have to combine weirdly but instead just have one clean
        # entry per prompt-line.

        # Filter all prompt but the "no prompt"
        prompts = df.loc[df[config.fields.prompt] != config.fields.no_prompt, config.fields.prompt].unique()

        # All entries of files that have not been generated by prompt (so all v0's)
        nop_rows = df[df[config.fields.prompt] == config.fields.no_prompt]

        # duplicate each "no prompt" entry for each prompt such that it is easier to handle the separate "prompt lines" of data
        # Note: This will throw an error if there is no data that has been generated via a prompt.
        expanded = pd.concat(
            [nop_rows.assign(Prompt=p) for p in prompts],
            ignore_index=True
        )

        # Construct new df
        df_final = pd.concat(
            [df[df[config.fields.prompt] != config.fields.no_prompt], expanded],
            ignore_index=True
        )

        df_final = df_final.sort_values(
            by=[config.fields.snippet, config.fields.variant, config.fields.version, config.fields.prompt])

        return df_final

    def load_comparison_data_to_df(self):
        """
        Function to read in the CSV file with the Comparison Data
        and process it into a dataframe.

        Expected columns: Snippet, Variant 1, Variant 2, Version 1, Version 2, Modifications, Insertions, Deletions

        :return: The dataframe.
        """

        # Choose CSV file that contain the absolute values
        filename = os.path.join(self.results_dir, f"{config.constants.COMPARISON_REPORT}.csv")

        if not os.path.exists(filename):
            raise FileNotFoundError(
                f"No CSV-file '{filename}' found.")


        logging.debug("Loading Comparison Values into dataframe...")

        # Read CSV file
        df = pd.read_csv(filename)

        required_cols = config.fields.COMPARISON_REPORT_COMPLETE_HEADER

        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"(Comparison Report) Column '{col}' not in DataFrame.")

        return df

    def load_change_types_data_to_df(self):

        # Choose CSV file that contain the change types
        filename = os.path.join(self.results_dir, f"{config.constants.CHANGE_TYPE_REPORT}.csv")

        if not os.path.exists(filename):
            raise FileNotFoundError(
                f"No CSV-file '{filename}' found.")


        logging.debug("Loading Change Report Values into dataframe...")

        # Read CSV file
        df = pd.read_csv(filename)

        required_cols = config.fields.CHANGE_TYPES_REPORT_COMPLETE_HEADER

        for col in required_cols:
            if col not in df.columns:
                raise ValueError(f"(Change Types) Column '{col}' not in DataFrame.")

        # Replace missing values in AST-columns with 'UNKNOWN'
        df[config.fields.source_ast_type] = df[config.fields.source_ast_type].fillna("UNKNOWN").astype(str)
        df[config.fields.target_ast_type] = df[config.fields.target_ast_type].fillna("UNKNOWN").astype(str)

        return df


    def save_output_to_csv_and_md(self, data, filename, index=False):
        """
        Function to save data into CSV and Markdown files.

        :param data: Pandas DataFrame that contains the data.
        :param filename: Name for the output file.
        :param output_dir: Path to output directory.
        :param index: Whether the table index should be included or not.
        """

        output_dir = self.analysis_dir

        data = data.copy()  # such that the original data is not changed in the following step

        # Save CSV files
        data.to_csv(os.path.join(output_dir, filename + '.csv'), index=index)

        # Save Markdown files
        md_path = os.path.join(output_dir, filename + '.md')

        # ---- FORMATTING FOR MARKDOWN FILES READABILITY ONLY!!!
        # If there are diff data for absolute values --> convert to % strings
        existing_cols = [col for col in config.fields.ABSOLUTE_VALUES_METRICS if col in data.columns]

        if existing_cols:
            for col in existing_cols:
                data[col] = (data[col] * 100).round(2)
                data[col] = data[col].map(lambda x: f"{x:+.2f}%")

        # Test if values are in percentage format in this dataframe and convert for better readability to % strings
        existing_percentage_cols = [col for col in config.fields.ALL_PERCENTAGE_COLUMNS if col in data.columns]

        if existing_percentage_cols:
            if data[existing_percentage_cols].apply(lambda s: s.between(0, 1).all()).all():
                for col in existing_percentage_cols:
                    data[col] = (data[col] * 100).round(2).astype(str) + "%"
        # --- END OF FORMATTING FOR MARKDOWN FILES

        with open(md_path, 'w', encoding="utf-8") as f:
            f.write(f"# {filename}\n\n")

            if config.fields.snippet in data.columns:
                for snippet_name, snippet_df in data.groupby(config.fields.snippet):
                    f.write(f"## {snippet_name}\n\n")
                    f.write(snippet_df.to_markdown(index=index))
                    f.write("\n\n")
            else:
                f.write(data.to_markdown(index=index))
                f.write("\n")

        logging.debug(f"Saved ‘{output_dir}/{filename} ’.")

    def save_multiple_to_markdown(self, datasets: dict, filename):
        """
        Saves multiple datasets in a Markdown file in separate tables with titles.

        :param datasets: A dictionary that contains (title, dataframe) items.
        :param filename: Name for the output file.
        """

        output_dir = self.analysis_dir

        md_path = os.path.join(output_dir, filename + '.md')

        with open(md_path, 'w', encoding="utf-8") as f:
            for title, data in datasets.items():
                # Write table title
                f.write(f"# {title}\n\n")

                # Format table as Markdown
                data_md = data.to_markdown(tablefmt="pipe", index=True)
                f.write(data_md + "\n\n")

        logging.debug(f"The analysis results were saved as CSV and Markdown ‘{output_dir}/{filename} ’.")

    ################################################################################################

    @staticmethod
    def analyze_differences(df_absolute: pd.DataFrame, df_comparison: pd.DataFrame) -> pd.DataFrame:
        """

        :param df_absolute:
        :param df_comparison:
        :return:
        """
        logging.info("Analyze differences in absolute values between two compared files.")

        result_rows = []

        for _, row in df_comparison.iterrows():

            # Values we need to find the matching entries in df_absolute
            snippet = row[config.fields.snippet]
            v1 = row[config.fields.variant1]
            v2 = row[config.fields.variant2]
            ver1 = row[config.fields.version1]
            ver2 = row[config.fields.version2]
            prompt_id = row[config.fields.prompt]

            # Find both entries in df_absolute
            row1_absolute = df_absolute[
                (df_absolute[config.fields.snippet] == snippet) &
                (df_absolute[config.fields.variant] == v1) &
                (df_absolute[config.fields.version] == ver1) &
                ( # If the base snippets are part of the comparision (v0), then the prompt_id is "config.no_prompt"
                        (df_absolute[config.fields.prompt] == prompt_id) |
                        ((ver1 == 0) & (prompt_id == config.fields.no_prompt))
                )
                ]

            row2_absolute = df_absolute[
                (df_absolute[config.fields.snippet] == snippet) &
                (df_absolute[config.fields.variant] == v2) &
                (df_absolute[config.fields.version] == ver2) &
                ( # If the base snippets are part of the comparision (v0), then the prompt_id is "config.no_prompt"
                        (df_absolute[config.fields.prompt] == prompt_id) |
                        ((ver2 == 0) & (prompt_id == config.fields.no_prompt))
                )
                ]

            if row1_absolute.empty or row2_absolute.empty:
                logging.warning(f"Missing entry for: {snippet}, {v1}_v{ver1} or {v2}_v{ver2}")
                continue

            row1_absolute = row1_absolute.iloc[0]
            row2_absolute = row2_absolute.iloc[0]

            loc_interval1 =  row1_absolute[config.fields.LOC_interval]
            loc_interval2 =  row2_absolute[config.fields.LOC_interval]

            assert(loc_interval1 == loc_interval2) # Note: Sonst ist irgendwo echt was schief gelaufen >.<

            # Calc relative diffs of absolute values between both files
            metric_diffs = {}

            for metric in config.fields.ABSOLUTE_VALUES_METRICS:

                old_val = row1_absolute[metric]
                new_val = row2_absolute[metric]

                if old_val == 0:
                    if new_val == 0:
                        diff = 0.0  # Keine Änderung
                    else:
                        diff = new_val
                        logging.warning(
                            f"Division by zero avoided for {metric} in {snippet}, {v1}_v{ver1} → {v2}_v{ver2}")
                else:
                    diff = (new_val - old_val) / old_val

                metric_diffs[metric] = diff

            # Build result row: all column from original comparison report + all new ones + LOC interval
            result_row = {col: row[col] for col in df_comparison.columns} | {config.fields.LOC_interval: loc_interval1}
            result_row.update(metric_diffs)

            result_rows.append(result_row)

        # Create new df from rows
        df_result = pd.DataFrame(result_rows)

        return df_result


    ################################################################################################

    @staticmethod
    def summarize_ast_changes(df: pd.DataFrame, per_diff: bool, n_examples=3):

        """
        Summarize AST-based code changes by aggregating counts, subcounts,
        and representative examples of source–target line pairs.

        This function takes a DataFrame of AST-level changes (as produced by a diff parser)
        and summarizes them either globally across all diffs or within individual diffs,
        depending on the `per_diff` flag.

        Each change is classified (e.g., `Rename`, `IdenticalPartialAST`, `OtherStructuralChange`),
        and aggregated both on the classification level and the combination of AST node types
        (`Source AST Type → Target AST Type`). Example code line pairs are collected to illustrate
        the types of changes.

        Args:
            df (pd.DataFrame): DataFrame containing AST-level code changes.
                Expected columns include:
                - Source AST Type
                - Target AST Type
                - Classification
                - Source Line
                - Target Line
                - Snippet, Variant1, Variant2, Version1, Version2, Prompt
            per_diff (bool): Determines the grouping scope.
                - If True, results are grouped by diff context
                  (i.e., [Snippet, Variant1, Variant2, Version1, Version2, Prompt]),
                  so counts and examples are reported per individual comparison.
                - If False, all changes are aggregated globally across the dataset.
            n_examples (int, optional): Maximum number of example line pairs to include
                per (Classification, AST-type combination). Defaults to 3.

        Returns:
            pd.DataFrame: A summarized DataFrame containing:
                - Grouping keys (depending on `per_diff`)
                - Classification
                - Count of changes (per classification, regardless of AST type)
                - AST type combination ("SourceType → TargetType")
                - Subcount of changes for that specific AST type combination
                - Example source→target line pairs (up to `n_examples`)

        Example:
            Input (excerpt):
                Source AST Type,Target AST Type,Classification,Source Line,Target Line,Snippet,Variant 1,Variant 2,Version 1,Version 2,Prompt
                if_statement,if_statement,Rename,if (arr[m] == x),if (arr[mid] == target) {,BinarySearch,KF0,KF0,0,1,pKF0

            Output (excerpt):
                Classification | Count | AST Type Combination | Subcount | Examples (n=3)
                -------------- | ----- | ------------------- | -------- | -------------------------------
                Rename         |   10  | if_statement → if_statement | 2 | "if (arr[m] == x) → if (arr[mid] == target) {"
        """

        df = df.copy()

        # Create a combined AST type string, e.g. "if_statement → if_statement"
        df[config.fields.ast_type_combination] = df.apply(
            lambda row: f"{config.ast_config.get_node_category(row[config.fields.source_ast_type])}"
            if row[config.fields.source_ast_type] == row[config.fields.target_ast_type]
            else f"{config.ast_config.get_node_category(row[config.fields.source_ast_type])} → {config.ast_config.get_node_category(row[config.fields.target_ast_type])}",
            axis=1
        )

        # If per_diff=True: group counts within each diff context
        # (Snippet, Variant, Version1, Version2, Prompt).
        # If False: aggregate across all diffs together.
        group_keys = config.fields.COMPARISON_REPORT_BASE_HEADER if per_diff else []

        # Count changes per group (optionally diff-level) and classification
        df[config.fields.count] = df.groupby(group_keys + [config.fields.classification])[
            config.fields.classification].transform("count")

        # Count changes per group, classification, and AST-type combination
        df[config.fields.subcount] = df.groupby(group_keys + [config.fields.classification,
                                                              config.fields.ast_type_combination])[
            config.fields.ast_type_combination].transform(
            "count")

        # Collect example source→target line pairs (up to n_examples per group)
        examples_df = (
            df.groupby(group_keys + [config.fields.classification, config.fields.ast_type_combination])[
                [config.fields.source_line, config.fields.target_line]
            ]
            .apply(lambda g: "<br>".join(
                g[[config.fields.source_line, config.fields.target_line]]
                .fillna("")
                .astype(str)
                .head(n_examples)
                .apply(lambda row: f"{row[config.fields.source_line]} → {row[config.fields.target_line]}", axis=1)
            ))
            .reset_index(name=f"Examples (n={n_examples})")
        )

        # Create final summary DataFrame
        final_df = (
            df[group_keys + [config.fields.classification, config.fields.count, config.fields.ast_type_combination,
                             config.fields.subcount]]
            .drop_duplicates()
            .merge(examples_df, on=group_keys + [config.fields.classification, config.fields.ast_type_combination], how="left")
        )

        # Sort result:
        # - If per_diff=True: sort within each diff by Classification and AST combination
        # - If per_diff=False: sort globally by count and subcount

        if per_diff:
            sort_by_keys = group_keys + [config.fields.count, config.fields.subcount]
            ascending_order = [True for _ in group_keys] + [False, False]
            #
            # sort_by_keys = [config.classification, config.ast_type_combination]
            # ascending_order = [False, False]

        else:
            sort_by_keys = [config.fields.count, config.fields.subcount]
            ascending_order = [False, False]

        final_df = final_df.sort_values(
            by=sort_by_keys,
            ascending=ascending_order
        ).reset_index(drop=True)

        return final_df

    ################################################################################################

    @staticmethod
    def convert_to_percentage_values(eval_diff_data: pd.DataFrame) -> pd.DataFrame:
        """
        Convert values of detailed insertions and deletions to percentage values:

            --> 100 % Code       = Unchanged + Modifications + Insertions + Deletions
                --> Deletions  = divided into deleted Code/Comment/Empty Lines
                --> Insertions = divided into inserted Code/Comment/Empty Lines


        :param eval_diff_data:
        :return:
        """
        df = eval_diff_data.copy()

        df["Total"] = eval_diff_data[config.fields.MAIN_CHANGE_COLS].sum(axis=1)

        # Calc percentage of unchanged vs. changed (modified) lines
        for col in config.fields.MAIN_CHANGE_COLS:
            df[col] = df[col] / df["Total"].replace(0, 1)

        # Calc percentage of detailed deletions
        for col in config.fields.DETAILED_DELS_COLS:
            df[col] = df[col] / df["Total"].replace(0, 1)

        # Calc percentage of detailed insertions
        for col in config.fields.DETAILED_INS_COLS:
            df[col] = df[col] / df["Total"].replace(0, 1)

        # Calc percentage of detailed modifications
        # NOTE: THIS ONLY WORKS IF WE HAVE ONE MAIN CLASSIFICATION PER LINE PAIR!!
        for col in config.fields.MODIFICATIONS_COLS:
            df[col] = df[col] / df["Total"].replace(0, 1)

        for col in config.fields.SEMANTIC_CHANGE_COLS:
            df[col] = df[col] / df["Total"].replace(0, 1)

        for col in config.fields.DEBUG_CATEGORIES:
            df[col] = df[col] / df["Total"].replace(0, 1)

        # Remove extra column again
        df = df.drop(columns=["Total"])

        return df

    @staticmethod
    def compute_minmax(data, metrics):
        """
        Calculates the largest and smallest differences for the specified metrics.

        :param data: Pandas DataFrame that contains the data.

        :return: Panda's DataFrame with the largest and smallest differences.
        """

        if config.fields.empty:
            return data

        logging.info(f"Compute minmax for {metrics}...")

        # Find largest and smallest differences per metric
        largest_differences = data[metrics].idxmax()
        smallest_differences = data[metrics].idxmin()

        # Prepare new results
        records = []

        for metric in metrics:
            # Largest differences
            largest_row = data.loc[largest_differences[metric]].copy()
            largest_row['Metric'] = f'Largest ({metric})'
            largest_row['Value'] = data.loc[largest_differences[metric], metric]

            # Convert to percentage values for better readability
            val = largest_row['Value'] * 100
            largest_row['Value'] = f"{val:+.2f}%" if 'avg_' + metric in config.fields.ABSOLUTE_VALUES_METRICS else f"{val:.2f}%"

            records.append(largest_row)

            # Smallest differences
            smallest_row = data.loc[smallest_differences[metric]].copy()
            smallest_row['Metric'] = f'Smallest ({metric})'
            smallest_row['Value'] = data.loc[smallest_differences[metric], metric]

            # Convert to percentage values for better readability
            val = (smallest_row['Value'] * 100)
            smallest_row['Value'] = f"{val:+.2f}%" if 'avg_' + metric in config.fields.ABSOLUTE_VALUES_METRICS else f"{val:.2f}%"


            records.append(smallest_row)

        # Create result data frame
        results = pd.DataFrame(records)



        # Only keep base columns + new columns
        base_cols = [col for col in config.fields.COMPARISON_REPORT_BASE_HEADER if col in data.columns]
        desired_columns = ['Metric', 'Value'] + base_cols
        results = results[desired_columns]

        # Set index
        results = results.set_index('Metric')

        return results

    ################################################################################################

    @staticmethod
    def filter_horizontal_entries(df: pd.DataFrame, direct_successors=False) -> pd.DataFrame:
        """Return df with only directly succeessing versions."""
        df = df.copy()

        df_filtered = df[
            (df[config.fields.variant1] == df[config.fields.variant2])
        ].copy()

        df_filtered = df_filtered.drop(columns=[config.fields.variant2])
        df_filtered = df_filtered.rename(columns={config.fields.variant1: config.fields.variant})

        if direct_successors:
            df_filtered = df_filtered[
                (df_filtered[config.fields.version2] == df_filtered[config.fields.version1] + 1)
            ]

        return df_filtered.reset_index(drop=True)

    @staticmethod
    def filter_vertical_entries(df: pd.DataFrame) -> pd.DataFrame:
        """Filters only entries with the same version number."""
        df = df.copy()

        df_filtered = df[
            (df[config.fields.version1] == df[config.fields.version2])
        ].copy()

        df_filtered = df_filtered.drop(columns=[config.fields.version2])
        df_filtered = df_filtered.rename(columns={config.fields.version1: config.fields.version})

        return df_filtered.reset_index(drop=True)


    ################################################################################################
    # *** Horizontal analysis ***
    ################################################################################################

    @staticmethod
    def horizontal_aggregation(df):
        """
        Files of the same variant are compared with each other with regard to the different versions.
        Calls function to save output of analysis as CSV and Markdown file.

        :param df: Pandas DataFrame that contains the data.
        :return: Pandas DataFrame that contains the analysis data with columns Variant, Version 1, Version 2, [all metrics]
        """

        logging.debug("Preparing horizontal summary...")

        # Filter only rows in which both variants are identical
        horizontal_changes = Evaluator.filter_horizontal_entries(df, direct_successors=False)

        # Group and calculate average values for horizontal changes
        horizontal_summary = horizontal_changes.groupby([config.fields.variant, config.fields.version1,
                                                         config.fields.version2, config.fields.prompt]).agg(
            **config.fields.avg_dict
        ).reset_index()

        return horizontal_summary

    ################################################################################################
    # *** Vertical Analysis ***
    ################################################################################################

    @staticmethod
    def vertical_aggregation(df):
        """
        Files of the same version are compared in terms of changes between the variants.
        Calls function to save output of analysis as CSV and Markdown file.

        :param df: Pandas DataFrame that contains the data.
        :return: Pandas DataFrame that contains the analysis data with columns Version, Variant 1, Variant 2, [all metrics]
        """

        logging.debug("Preparing vertical summary...")

        # Filter: Only rows in which the versions are identical
        vertical_changes = Evaluator.filter_vertical_entries(df)

        # Group and calculate average values for vertical changes
        vertical_summary = vertical_changes.groupby([config.fields.version, config.fields.variant1,
                                                     config.fields.variant2, config.fields.prompt]).agg(
            **config.fields.avg_dict
        ).reset_index()

        return vertical_summary



    ################################################################################################
    # *** _____ ***
    ################################################################################################
    @staticmethod
    def _add_interpretation(df, directed=False):
        """Hilfsfunktion: fügt Interpretationsspalte für Tabellen hinzu."""
        interpretations = []
        for _, row in df.iterrows():
            target = row["Change Type"]
            variant = row["Code Variant"]
            p = row.get("Corrected p-value (FDR)", row.get("p-value", None))
            test = row["Statistical Test"]

            if pd.isna(p):
                interpretations.append("Test could not be computed")
                continue

            if directed:
                if p < 0.05:
                    interpretations.append(
                        f"p < 0.05, Prompts significantly affected {target} changes in {variant} ({test})"
                    )
                else:
                    interpretations.append(
                        f"p > 0.05, No significant effect of prompts on {target} in {variant}"
                    )
            else:
                if p < 0.05:
                    interpretations.append(
                        f"p < 0.05, Prompt choice significantly affects {target} ({test})"
                    )
                else:
                    interpretations.append(
                        f"p > 0.05, No effect of prompts on {target} detected"
                    )
        df["Interpretation"] = interpretations
        return df

    def aggregate_and_plot_change_types(self, df, group_cols, title_prefix):
        """
        Gruppiert nach group_cols und aggregiert Change-Types.
        Exportiert Ergebnisse als CSV, MD und PNG.
        Zusätzlich: statistische Tests zu RQ3 (Prompt-Effekt).
          - Deskriptive Statistiken (M ± SD)
          - Directed Tests für Rename & CommentChange
          - Explorative Tests für alle Change Types (mit multipler Testkorrektur)
        """

        change_types = config.fields.MODIFICATIONS_COLS

        output_dir = Path(self.analysis_dir, "groups_analysis")
        os.makedirs(output_dir, exist_ok=True)

        df = df.copy()

        # --- Prozentwerte berechnen --- SIND SCHON IN PROZENT
        # for col in change_types:
        #     df[col] = df[col] / df[config.modifications].replace(0, 1)

        grouped = df.groupby(group_cols)[change_types].mean()
        grouped = grouped.div(grouped.sum(axis=1), axis=0)

        most_common = grouped.idxmax(axis=1)
        least_common = grouped.idxmin(axis=1)

        summary = pd.DataFrame({
            "Most Common": most_common,
            "Least Common": least_common
        })

        safe_name = "_".join(group_cols).replace(" ", "_")
        csv_file = output_dir / f"{title_prefix}_{safe_name}.csv"
        md_file = output_dir / f"{title_prefix}_{safe_name}.md"
        plot_file = output_dir / f"{title_prefix}_{safe_name}.png"

        # --- CSV exportieren ---
        grouped.to_csv(csv_file)
        summary.to_csv(output_dir / f"{title_prefix}_{safe_name}_summary.csv")

        # --- Markdown vorbereiten ---
        grouped_percent = grouped * 100
        grouped_fmt = grouped_percent.map(lambda x: f"{x:.2f}%")

        md_lines = []
        md_lines.append(f"# {title_prefix} grouped by {group_cols}\n")
        md_lines.append("## Aggregated Change Types (percentages)\n")
        md_lines.append(grouped_fmt.to_markdown())
        md_lines.append("\n## Dominant Change Types per Group\n")
        md_lines.append(summary.to_markdown())

        # --- Markdown schreiben ---
        with open(md_file, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))

        # --- Plot speichern ---
        plt.figure(figsize=(12, 6))
        sns.heatmap(grouped_percent, annot=True, fmt=".2f", cmap="Blues")
        plt.title(f"{title_prefix} Change Type Distribution (%)")
        plt.ylabel(" / ".join(group_cols))
        plt.xlabel("Change Types")
        plt.tight_layout()
        plt.savefig(plot_file)
        plt.close()

        logging.debug(f"[OK] Exported groups_analysis → {md_file}, {csv_file}")


    def run_rq3_analysis(self, df):

        # ----------------------------
        # RQ3-Analysen
        # ----------------------------

        # Warnungen unterdrücken (nur MixedLM)
        warnings.simplefilter("ignore", ConvergenceWarning)

        change_types = [c.replace("(", "_").replace(")", "") for c in config.fields.MODIFICATIONS_COLS]

        output_dir = Path(self.analysis_dir, "statistical_analysis")
        os.makedirs(output_dir, exist_ok=True)

        df = df.copy()

        # --- Spaltennamen bereinigen (Klammern etc. entfernen) ---
        df = df.rename(columns=lambda c: c.replace("(", "_").replace(")", ""))

        rq3_targets = [config.fields.name_change, config.fields.comment_change]

        directed_results = []
        exploratory_results = []

        for target in change_types:
            # Variante-weise Kruskal-Wallis
            grouped = df.groupby(config.fields.variant)
            for variant, sub in grouped:
                groups = [sub[sub[config.fields.prompt] == p][target].dropna()
                          for p in sub[config.fields.prompt].unique()]
                if len(groups) > 1:
                    stat, pval = stats.kruskal(*groups)
                    row = {
                        "Target": target,
                        config.fields.variant: variant,
                        "Test": "Kruskal-Wallis",
                        "stat": stat,
                        "pval": pval
                    }
                    exploratory_results.append(row)
                    if target in rq3_targets:
                        directed_results.append(row)

                    # Post-hoc paarweise Tests
                    for p1, p2 in combinations(sub[config.fields.prompt].unique(), 2):
                        g1 = sub[sub[config.fields.prompt] == p1][target].dropna()
                        g2 = sub[sub[config.fields.prompt] == p2][target].dropna()
                        if len(g1) > 0 and len(g2) > 0:
                            u, p = stats.mannwhitneyu(g1, g2, alternative="two-sided")
                            row = {
                                "Target": target,
                                config.fields.variant: variant,
                                "Test": f"Post-hoc {p1} vs {p2}",
                                "stat": u,
                                "pval": p
                            }
                            exploratory_results.append(row)
                            if target in rq3_targets:
                                directed_results.append(row)

            # Mixed Effects Model (Prompt-Effekt über alle Varianten)
            try:
                model = mixedlm(f"{target} ~ C(Prompt)", df, groups=df[config.fields.variant])
                result = model.fit()
                row = {
                    "Target": target,
                    config.fields.variant: "ALL",
                    "Test": "MixedLM",
                    "stat": result.llf,
                    "pval": result.pvalues.get("C(Prompt)[T.pKF1]", float("nan"))
                }
                exploratory_results.append(row)
                if target in rq3_targets:
                    directed_results.append(row)
            except Exception as e:
                logging.warning(f"[WARN] MixedLM for {target} failed: {e}")

        # --- P-Wert-Korrektur für explorative Tests ---
        if exploratory_results:
            expl_df = pd.DataFrame(exploratory_results)
            corrected = multipletests(expl_df["pval"], method="fdr_bh")
            expl_df["pval_corrected"] = corrected[1]
        else:
            expl_df = pd.DataFrame()

        dir_df = pd.DataFrame(directed_results)

        # Spalten sauber umbenennen
        dir_df = dir_df.rename(columns={
            "Target": "Change Type",
            config.fields.variant: "Code Variant",
            "Test": "Statistical Test",
            "stat": "Test Statistic",
            "pval": "p-value"
        })
        expl_df = expl_df.rename(columns={
            "Target": "Change Type",
            config.fields.variant: "Code Variant",
            "Test": "Statistical Test",
            "stat": "Test Statistic",
            "pval": "p-value",
            "pval_corrected": "Corrected p-value (FDR)"
        })

        # Interpretationen hinzufügen
        #dir_df = Evaluator._add_interpretation(dir_df, directed=True)
        #expl_df = Evaluator._add_interpretation(expl_df, directed=False)

        rq3_csv_dir = output_dir / f"rq3_directed.csv"
        rq3_csv_expl = output_dir / f"rq3_exploratory.csv"

        dir_df.to_csv(rq3_csv_dir, index=False)
        expl_df.to_csv(rq3_csv_expl, index=False)

        # ----------------------------
        # Deskriptive Statistik (M ± SD)
        # ----------------------------
        desc = df.groupby([config.fields.variant, config.fields.prompt])[change_types].agg(["mean", "std"]).round(3)
        desc_formatted = desc.copy()
        for col in change_types:
            desc_formatted[(col, "mean±std")] = desc[col]["mean"].astype(str) + " ± " + desc[col]["std"].astype(str)
        desc_table = desc_formatted.xs("mean±std", axis=1, level=1)

        rq3_desc = output_dir / f"rq3_desc.csv"
        desc_table.to_csv(rq3_desc)

        # ----------------------------
        # Markdown anhängen
        # ----------------------------
        md_file = output_dir / f"rq3_complete.md"
        md_lines = []

        md_lines.append("\n## Descriptive Statistics (Mean ± SD)\n")
        # For better readability in .md: convert to %-values in
        for col in change_types:
            desc_table[(col)] = (
                                (desc[col]["mean"] * 100).round(1).astype(str) + "% ± " +
                                (desc[col]["std"] * 100).round(1).astype(str) + "%"
                                )
        md_lines.append(desc_table.to_markdown())

        md_lines.append("\n## RQ3 Directed Tests (Rename & CommentChange)\n")
        if not dir_df.empty:
            md_lines.append(dir_df.to_markdown())
        else:
            md_lines.append("_No valid directed test results._")

        md_lines.append("\n## Exploratory Tests (All Change Types, FDR-corrected)\n")
        if not expl_df.empty:
            md_lines.append(expl_df.to_markdown())
        else:
            md_lines.append("_No valid exploratory test results._")

        # --- Markdown schreiben ---
        with open(md_file, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))

        logging.debug(f"[OK] Exported statistical results → {md_file}, {rq3_csv_dir}, {rq3_csv_expl}, {rq3_desc}")

    def merge_eval_diff_and_change_types_data(self, df_absolute_values, df_comparison_data, df_summarized_change_types):

        eval_diff_data = self.analyze_differences(df_absolute_values, df_comparison_data)

        # 1. Classification-Spalten erzeugen
        pivoted = (
            df_summarized_change_types
            .pivot_table(
                index=config.fields.COMPARISON_REPORT_BASE_HEADER,
                columns=config.fields.classification,
                values=config.fields.count,
                fill_value=0
            )
            .astype(int)
            .reindex(columns=config.fields.MODIFICATIONS_COLS + config.fields.SEMANTIC_CHANGE_COLS + config.fields.DEBUG_CATEGORIES, fill_value=0)  # <- it might be that not all classification types occur in one dataset, but for concistency I want to include all columns in df
            .reset_index()
        )

        pivoted[config.fields.semantic_change] = (
                pivoted[config.fields.SEMANTIC_CHANGE_COLS].sum(axis=1).astype(int)
        )

        pivoted[config.fields.other] = (
                pivoted[config.fields.DEBUG_CATEGORIES].sum(axis=1).astype(int)
        )

        # 2. Optional: Spaltennamen "flatten" (Pivot erzeugt evtl. MultiIndex)
        pivoted.columns.name = None

        # 3. Merge mit eval_diff_data
        merged = eval_diff_data.merge(
            pivoted,
            on=config.fields.COMPARISON_REPORT_BASE_HEADER,
            how="left"
        )

        # 4. Nullen für fehlende Klassen einsetzen
        merged[config.fields.ALL_PERCENTAGE_COLUMNS] = merged[
            config.fields.ALL_PERCENTAGE_COLUMNS].fillna(0).astype(int)

        required_cols = config.fields.EVAL_DIFF_METRICS

        for col in required_cols:
            if col not in merged.columns:
                raise ValueError(f"(Merged Eval Diff Data) Column '{col}' not in DataFrame.")

        # 5. Konsistenzprüfung: Summe der Klassifikationen == Modifications
        merged["classification_sum"] = merged[config.fields.MODIFICATIONS_COLS].sum(axis=1)

        inconsistent = merged[merged[config.fields.modifications] != merged["classification_sum"]]
        if not config.fields.empty:
            raise ValueError(
                f"Inconsistent data: {len(inconsistent)} rows where "
                f"{config.fields.modifications} != sum({config.fields.MODIFICATIONS_COLS}).\n"
                f"Examples:\n{inconsistent.head()}"
            )

        # Optional: Hilfsspalte wieder entfernen
        merged = merged.drop(columns=["classification_sum"])

        merged[config.fields.semantic_change] = (
            merged[config.fields.SEMANTIC_CHANGE_COLS].sum(axis=1).astype(int)
        )

        return merged

    def compute_change_type_ranking(self, df, prompt_specific=False):

        def classify_effect_strength(median_diff, r=None):
            abs_r = abs(r) if r is not None else 0

            if median_diff >= 0.05 and abs_r >= 0.3:
                return "Substantial negative impact (highly detrimental)"
            elif median_diff >= 0.03 and abs_r >= 0.2:
                return "Moderate negative impact (moderately detrimental)"
            elif median_diff >= 0.01 and abs_r >= 0.1:
                return "Minor negative impact (cosmetic)"
            elif median_diff > -0.01:
                return "Negligible impact"
            else:
                return "Potentially beneficial / artefactual"

        change_cols = sorted([c for c in config.fields.MODIFICATIONS_COLS if c not in [config.fields.other]])

        results = []

        if prompt_specific:
            for prompt, gdf in df.groupby(config.fields.prompt):
                if prompt == config.fields.no_prompt:
                    continue

                for c in change_cols:
                    if c not in gdf:
                        continue
                    try:
                        # Median similarity No vs Yes
                        no_vals = gdf.loc[gdf[c] == 0, config.fields.avg_similarity_score]
                        yes_vals = gdf.loc[gdf[c] > 0, config.fields.avg_similarity_score]
                        if len(no_vals) == 0 or len(yes_vals) == 0:
                            continue

                        median_no = no_vals.median()
                        median_yes = yes_vals.median()
                        diff = median_no - median_yes

                        # Correlation
                        if gdf[c].nunique() > 1:
                            r = gdf[c].corr(gdf[config.fields.avg_similarity_score])
                        else:
                            r = None

                        results.append({
                            config.fields.prompt: prompt,
                            "Median_Diff": diff,
                            "ChangeType": c,
                            "Median_No": median_no,
                            "Median_Yes": median_yes,
                            "Correlation_r": r,
                            "Impact_Classification": classify_effect_strength(diff, r)
                        })
                    except Exception:
                        continue

            df_rank = pd.DataFrame(results)
            df_rank = df_rank.sort_values(by="Median_Diff", ascending=False)
            fname_csv = os.path.join(self.analysis_dir, "change_type_ranking_per_prompt.csv")
            fname_md = os.path.join(self.analysis_dir, "change_type_ranking_per_prompt.md")
            df_rank.to_csv(fname_csv, index=False)
            df_rank.to_markdown(fname_md, index=False)
            logging.debug(f"Ranking table per prompt saved: {fname_csv}/.md")

        else:
            for c in change_cols:
                if c not in df:
                    continue
                try:
                    no_vals = df.loc[df[c] == 0, config.fields.avg_similarity_score]
                    yes_vals = df.loc[df[c] > 0, config.fields.avg_similarity_score]
                    if len(no_vals) == 0 or len(yes_vals) == 0:
                        continue

                    median_no = no_vals.median()
                    median_yes = yes_vals.median()
                    diff = median_no - median_yes

                    if df[c].nunique() > 1:
                        r = df[c].corr(df[config.fields.avg_similarity_score])
                    else:
                        r = None

                    results.append({
                        "ChangeType": c,
                        "Median_Diff": diff,
                        "Median_No": median_no,
                        "Median_Yes": median_yes,
                        "Correlation_r": r,
                        "Impact_Classification": classify_effect_strength(diff, r)
                    })
                except Exception:
                    continue

            df_rank = pd.DataFrame(results)
            df_rank = df_rank.sort_values(by="Median_Diff", ascending=False)
            fname_csv = os.path.join(self.analysis_dir, "change_type_ranking.csv")
            fname_md = os.path.join(self.analysis_dir, "change_type_ranking.md")
            df_rank.to_csv(fname_csv, index=False)
            df_rank.to_markdown(fname_md, index=False)
            logging.debug(f"Global ranking table saved: {fname_csv} (/.md)")


    def process_change_types(self, df_change_types):

        # Summarize AST changes

        # First, we need to preprocess the change types report, which lists the change type data
        # = [Source AST Type,Target AST Type,Classification,Source Line,Target Line] for each
        # LineDiff object = [Snippet,Variant 1,Variant 2,Version 1,Version 2,Prompt].
        # => We summarize AST-based code changes by aggregating counts, subcounts,
        #    and representative examples of source–target line pairs.

        df_summarized_change_types_per_diff = self.summarize_ast_changes(df_change_types, per_diff=True, n_examples=5)
        df_summarized_change_types_only_successors = self.filter_horizontal_entries(df_summarized_change_types_per_diff,
                                                                                    True)

        # Filter and aggregate
        df_change_types_filtered = self.filter_horizontal_entries(df_change_types, True)
        df_summarized_change_types_across_all = self.summarize_ast_changes(df_change_types_filtered, per_diff=False,
                                                                           n_examples=5)

        # Save outputs
        self.save_output_to_csv_and_md(df_summarized_change_types_per_diff, "Change_Types_Summary_Per_Diff_Filtered")
        self.save_output_to_csv_and_md(df_summarized_change_types_only_successors,
                                       "Change_Types_Summary_Only_Successors")
        self.save_output_to_csv_and_md(df_summarized_change_types_across_all,
                                       "Change_Types_Summary_Across_All_Filtered")

        return df_summarized_change_types_per_diff, df_summarized_change_types_only_successors, df_summarized_change_types_across_all

    def merge_and_convert(self, df_absolute_values, df_comparison_data, df_summarized_change_types_per_diff):

        merged_eval_diff_data = self.merge_eval_diff_and_change_types_data(
            df_absolute_values, df_comparison_data, df_summarized_change_types_per_diff
        )
        merged_eval_diff_data_in_percent = self.convert_to_percentage_values(merged_eval_diff_data)

        self.save_output_to_csv_and_md(merged_eval_diff_data, "Eval_Diff_Data")
        self.save_output_to_csv_and_md(merged_eval_diff_data_in_percent, "Eval_Diff_Data_Percentages")

        return merged_eval_diff_data, merged_eval_diff_data_in_percent


    def filter_groups(self, merged_eval_diff_data, merged_eval_diff_data_in_percent):
        # Horizontal filters
        only_direct_successors = self.filter_horizontal_entries(merged_eval_diff_data, True)
        only_direct_successors_percentage = self.filter_horizontal_entries(merged_eval_diff_data_in_percent, True)

        self.save_output_to_csv_and_md(only_direct_successors, "Eval_Diff_Data_Only_Direct_Successors")
        self.save_output_to_csv_and_md(only_direct_successors_percentage,
                                       "Eval_Diff_Data_Only_Direct_Successors_Percentage")

        # Vertical filters
        vertical_entries = self.filter_vertical_entries(merged_eval_diff_data)
        vertical_entries_in_percent = self.filter_vertical_entries(merged_eval_diff_data_in_percent)

        self.save_output_to_csv_and_md(vertical_entries, "Eval_Diff_Data_Vertical_Groups")
        self.save_output_to_csv_and_md(vertical_entries_in_percent, "Eval_Diff_Data_Vertical_Groups_Percentage")

        return only_direct_successors, only_direct_successors_percentage, vertical_entries, vertical_entries_in_percent

    def run_aggregations(self, merged_eval_diff_data_in_percent):
        horizontal_data = self.horizontal_aggregation(merged_eval_diff_data_in_percent)
        vertical_data = self.vertical_aggregation(merged_eval_diff_data_in_percent)

        return horizontal_data, vertical_data


    def run_minmax_analysis(self, merged_eval_diff_data_in_percent, horizontal_data, vertical_data):
        min_max_comp = self.compute_minmax(merged_eval_diff_data_in_percent, metrics=config.fields.EVAL_DIFF_METRICS)
        min_max_horizontal = self.compute_minmax(horizontal_data, metrics=config.fields.AVG_METRICS)
        min_max_vertical = self.compute_minmax(vertical_data, metrics=config.fields.AVG_METRICS)

        self.save_output_to_csv_and_md(min_max_horizontal, "Min_Max_Horizontal_Analysis", index=True)
        self.save_output_to_csv_and_md(min_max_vertical, "Min_Max_Vertical_Analysis", index=True)
        self.save_output_to_csv_and_md(min_max_comp, "Min_Max_All_Singles", index=True)



    def run_analysis(self):
        logging.info(f"[Evaluator] Run analysis for {self.results_dir}!")

        # # 1. Load all data, i.e. convert CSV files to df

        df_absolute_values = self.load_absolute_values_to_df()
        df_comparison_data = self.load_comparison_data_to_df()
        df_change_types = self.load_change_types_data_to_df()

        # 2. Change types preprocessing
        df_summarized_change_types_per_diff, df_summarized_change_types_only_successors, df_summarized_change_types_across_all = \
            self.process_change_types(df_change_types)

        # 3. Merge + percentages
        merged_eval_diff_data, merged_eval_diff_data_percentage = \
            self.merge_and_convert(df_absolute_values, df_comparison_data, df_summarized_change_types_per_diff)

        # 4. Filter groups (horizontal/vertical)
        only_direct_successors, only_direct_successors_percentage, vertical_entries, vertical_entries_in_percent = \
            self.filter_groups(merged_eval_diff_data, merged_eval_diff_data_percentage)


        #----------------------------
        # If you have loaded, merged and saved the data before, but want to run
        # more evaluations or visualizations, work with the generated files:

        # Example paths:
        # merged_eval_diff_data_percentage = pd.read_csv("../output/main-experiment/LOC_50-200/analysis/Eval_Diff_Data_Percentages.csv")
        # only_direct_successors_percentage = pd.read_csv(
        #     "../output/main-experiment/LOC_50-200/analysis/Eval_Diff_Data_Only_Direct_Successors_Percentage.csv")
        # vertical_entries_in_percent = pd.read_csv(
        #     "../output/main-experiment/LOC_50-200/analysis/Eval_Diff_Data_Vertical_Groups_Percentage.csv")
        #df_summarized_change_types_per_diff = pd.read_csv("../output/main-experiment/LOC_50-200/analysis/Change_Types_Summary_Per_Diff_Filtered.csv")

        # 5. Aggregations & Groups Analysis on Change Type Frequencies
        horizontal_data, vertical_data = self.run_aggregations(merged_eval_diff_data_percentage)

        return [only_direct_successors_percentage, merged_eval_diff_data_percentage, horizontal_data, vertical_data, df_absolute_values, vertical_entries_in_percent, df_summarized_change_types_per_diff]
    

    def plot_results(self, only_direct_successors_percentage, merged_eval_diff_data_percentage, horizontal_data, vertical_data, df_absolute_values, vertical_entries_in_percent, df_summarized_change_types_per_diff):
        
        # Export group analyses
        self.aggregate_and_plot_change_types(only_direct_successors_percentage, [config.fields.prompt], "")
        self.aggregate_and_plot_change_types(only_direct_successors_percentage, [config.fields.variant,
                                                                                 config.fields.prompt], "")

        # Run statistical analysis for RQ3
        self.run_rq3_analysis(only_direct_successors_percentage)
        va.plot_rq3_data(self.results_dir, [config.fields.name_change, config.fields.comment_change])

        # 6. Min/Max analysis
        self.run_minmax_analysis(merged_eval_diff_data_percentage, horizontal_data, vertical_data)

        # 7. Plots
        va.plot_absolute_values(data=df_absolute_values, output_dir=self.results_dir)
        va.plot_all_horizontal_stacked_change_distributions(diff_data=only_direct_successors_percentage,
                                                             output_dir=self.results_dir)
        va.plot_all_vertical_stacked_change_distributions(diff_data=vertical_entries_in_percent,
                                                          output_dir=self.results_dir)

        va.sankey_plot(only_direct_successors_percentage, output_dir=self.results_dir)

        va.histplot_plot(only_direct_successors_percentage, output_dir=self.results_dir)

        # These two currently only plot avg simscore heatmaps
        va.plot_horizontal_data(data=horizontal_data, output_dir=self.results_dir)
        va.plot_vertical_data(data=vertical_data, output_dir=self.results_dir)

        # NEW:
        self.compute_change_type_ranking(merged_eval_diff_data_percentage, prompt_specific=True)
        self.compute_change_type_ranking(merged_eval_diff_data_percentage, prompt_specific=False)

        va.sunburst_plots(df_summarized_change_types_per_diff, self.results_dir)

        # ------- Additional (but not ready yet plots) ---------

        # Scatter plots - look nice!
        va.analyze_similarity_vs_percent_modified(merged_eval_diff_data_percentage, self.results_dir, prompt_specific=True)
        va.analyze_similarity_vs_percent_modified(merged_eval_diff_data_percentage, self.results_dir, prompt_specific=False)

        # Boxplots - also
        va.analyze_similarity_by_change_types(merged_eval_diff_data_percentage, self.results_dir, prompt_specific=False)
        va.analyze_similarity_by_change_types(merged_eval_diff_data_percentage, self.results_dir, prompt_specific=True)

        # Collect all relevant files for the paper and move them in one directory


def main():
    parser = argparse.ArgumentParser(description="Process all generated comparison data.")

    parser.add_argument(
        "folder",
        type=Path,
        help="Path to the folder containing the generated data."
    )

    args = parser.parse_args()

    folder_path = args.folder

    # Überprüfen, ob der Pfad existiert und ein Verzeichnis ist
    if not folder_path.exists() or not folder_path.is_dir():
        raise ValueError(f"{folder_path} is not a valid directory")

    if True:
        evaluator = Evaluator(str(folder_path))

        # check whether pickled data exists
        pickle_path = os.path.join(folder_path, "evaluator_data.pkl")
        if os.path.exists(pickle_path):
            logging.info("Loading pickled data...")
            with open(pickle_path, "rb") as f:
                data = pickle.load(f)
        else:
            logging.info("No pickled data found, running full analysis...")
            data = evaluator.run_analysis()

            # store data in a pickle file for later use
            with open(os.path.join(folder_path, "evaluator_data.pkl"), "wb") as f:
                pickle.dump(data, f)

        evaluator.plot_results(*data)

    logging.info("=== COLLECT PLOTS FOR PAPER ===")
    plots = [
        "plots/global/absolute_values_across_all-snippets_pKF0_vs_pKF1.pdf",
        "plots/global/absolute_values_across_all-snippets_pKF0_vs_pKF2.pdf",

        "plots/prompts/nop/vertical_plots/stacked_plots/v0_detailed_average_changes.pdf",

        "plots/prompts/pKF0/absolute_values_across_all-snippets_pKF0_all.pdf",
        "plots/prompts/pKF0/absolute_values_across_all-snippets_pKF0_KF0.pdf",
        "plots/prompts/pKF0/horizontal_plots/stacked_plots/KF0_pKF0_average_changes.pdf",
        "plots/prompts/pKF0/horizontal_plots/stacked_plots/KF0_pKF0_detailed_average_changes.pdf",
        "plots/prompts/pKF0/horizontal_plots/stacked_plots/KF1_pKF0_average_changes.pdf",
        "plots/prompts/pKF0/horizontal_plots/stacked_plots/KF1_pKF0_detailed_average_changes.pdf",
        "plots/prompts/pKF0/horizontal_plots/stacked_plots/KF2_pKF0_average_changes.pdf",
        "plots/prompts/pKF0/horizontal_plots/stacked_plots/KF2_pKF0_detailed_average_changes.pdf",
        "plots/prompts/pKF0/horizontal_plots/sankey/KF0_pKF0_sankey.pdf",
        "plots/prompts/pKF0/horizontal_plots/sankey/KF1_pKF0_sankey.pdf",
        "plots/prompts/pKF0/horizontal_plots/sankey/KF2_pKF0_sankey.pdf",
        "plots/prompts/pKF0/horizontal_plots/histplot/KF0_pKF0_histplot.pdf",
        "plots/prompts/pKF0/horizontal_plots/histplot/KF1_pKF0_histplot.pdf",
        "plots/prompts/pKF0/horizontal_plots/histplot/KF2_pKF0_histplot.pdf",
        "plots/prompts/pKF0/horizontal_plots/heatmaps/avg_simscore_heatmap_KF0_pKF0.pdf",
        "plots/prompts/pKF0/horizontal_plots/heatmaps/avg_simscore_heatmap_KF1_pKF0.pdf",
        "plots/prompts/pKF0/horizontal_plots/heatmaps/avg_simscore_heatmap_KF2_pKF0.pdf",
        "plots/prompts/pKF0/vertical_plots/avg_simscore_lineplot_pKF0.pdf",

        "plots/prompts/pKF1/absolute_values_across_all-snippets_pKF1_all.pdf",
        "plots/prompts/pKF1/absolute_values_across_all-snippets_pKF1_KF0.pdf",
        "plots/prompts/pKF1/horizontal_plots/stacked_plots/KF0_pKF1_average_changes.pdf",
        "plots/prompts/pKF1/horizontal_plots/stacked_plots/KF0_pKF1_detailed_average_changes.pdf",
        "plots/prompts/pKF1/horizontal_plots/stacked_plots/KF1_pKF1_average_changes.pdf",
        "plots/prompts/pKF1/horizontal_plots/stacked_plots/KF1_pKF1_detailed_average_changes.pdf",
        "plots/prompts/pKF1/horizontal_plots/stacked_plots/KF2_pKF1_average_changes.pdf",
        "plots/prompts/pKF1/horizontal_plots/stacked_plots/KF2_pKF1_detailed_average_changes.pdf",
        "plots/prompts/pKF1/horizontal_plots/sankey/KF0_pKF1_sankey.pdf",
        "plots/prompts/pKF1/horizontal_plots/sankey/KF1_pKF1_sankey.pdf",
        "plots/prompts/pKF1/horizontal_plots/sankey/KF2_pKF1_sankey.pdf",
        "plots/prompts/pKF1/horizontal_plots/histplot/KF0_pKF1_histplot.pdf",
        "plots/prompts/pKF1/horizontal_plots/histplot/KF1_pKF1_histplot.pdf",
        "plots/prompts/pKF1/horizontal_plots/histplot/KF2_pKF1_histplot.pdf",
        "plots/prompts/pKF1/horizontal_plots/heatmaps/avg_simscore_heatmap_KF0_pKF1.pdf",
        "plots/prompts/pKF1/horizontal_plots/heatmaps/avg_simscore_heatmap_KF1_pKF1.pdf",
        "plots/prompts/pKF1/horizontal_plots/heatmaps/avg_simscore_heatmap_KF2_pKF1.pdf",
        "plots/prompts/pKF1/vertical_plots/avg_simscore_lineplot_pKF1.pdf",

        "plots/prompts/pKF2/absolute_values_across_all-snippets_pKF2_all.pdf",
        "plots/prompts/pKF2/absolute_values_across_all-snippets_pKF2_KF0.pdf",
        "plots/prompts/pKF2/horizontal_plots/stacked_plots/KF0_pKF2_average_changes.pdf",
        "plots/prompts/pKF2/horizontal_plots/stacked_plots/KF0_pKF2_detailed_average_changes.pdf",
        "plots/prompts/pKF2/horizontal_plots/stacked_plots/KF1_pKF2_average_changes.pdf",
        "plots/prompts/pKF2/horizontal_plots/stacked_plots/KF1_pKF2_detailed_average_changes.pdf",
        "plots/prompts/pKF2/horizontal_plots/stacked_plots/KF2_pKF2_average_changes.pdf",
        "plots/prompts/pKF2/horizontal_plots/stacked_plots/KF2_pKF2_detailed_average_changes.pdf",
        "plots/prompts/pKF2/horizontal_plots/sankey/KF0_pKF2_sankey.pdf",
        "plots/prompts/pKF2/horizontal_plots/sankey/KF1_pKF2_sankey.pdf",
        "plots/prompts/pKF2/horizontal_plots/sankey/KF2_pKF2_sankey.pdf",
        "plots/prompts/pKF2/horizontal_plots/histplot/KF0_pKF2_histplot.pdf",
        "plots/prompts/pKF2/horizontal_plots/histplot/KF1_pKF2_histplot.pdf",
        "plots/prompts/pKF2/horizontal_plots/histplot/KF2_pKF2_histplot.pdf",
        "plots/prompts/pKF2/horizontal_plots/heatmaps/avg_simscore_heatmap_KF0_pKF2.pdf",
        "plots/prompts/pKF2/horizontal_plots/heatmaps/avg_simscore_heatmap_KF1_pKF2.pdf",
        "plots/prompts/pKF2/horizontal_plots/heatmaps/avg_simscore_heatmap_KF2_pKF2.pdf",
        "plots/prompts/pKF2/vertical_plots/avg_simscore_lineplot_pKF2.pdf",
    ]

    for plot in plots:
        src = os.path.join(folder_path, plot)
        dst = os.path.join(folder_path, "paper_plots", os.path.basename(plot))
        os.makedirs(os.path.dirname(dst), exist_ok=True)
        if os.path.exists(src):
            shutil.copy(src, dst)
            logging.info(f"Copied {src} to {dst}")
        else:
            logging.warning(f"Plot not found: {src}")
            

    logging.info("=== DONE ===")

if __name__ == "__main__":
    logging.info("Running Evaluator.py ...")
    main()
