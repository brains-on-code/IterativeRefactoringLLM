{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CP_SEP = \";\" if os.name == \"nt\" else \":\"\n",
    "\n",
    "def compile_case(case_dir, classpath):\n",
    "    java_files = list(Path(case_dir).glob(\"*.java\"))\n",
    "    \n",
    "    #print(f\"Compiling Java files in {case_dir} with classpath {classpath}\")\n",
    "\n",
    "    cmd = [\n",
    "        \"javac\",\n",
    "        \"-cp\", f\"{classpath}{CP_SEP}{case_dir}\",\n",
    "        \"-d\", \".\",\n",
    "        *map(str, java_files)\n",
    "    ]\n",
    "\n",
    "    return subprocess.run(\n",
    "        cmd,\n",
    "        cwd=case_dir,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe710d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests(case_dir, classpath):\n",
    "    #print(f\"Running tests in {case_dir} with classpath {classpath}\")\n",
    "    \n",
    "    cmd = [\n",
    "        \"java\",\n",
    "        \"-cp\", f\"{classpath}{CP_SEP}{case_dir}\",\n",
    "        \"org.junit.platform.console.ConsoleLauncher\",\n",
    "        \"execute\", # Explicitly tell it to execute\n",
    "        \"--scan-class-path\",\n",
    "        \"--reports-dir\", str(case_dir)\n",
    "    ]\n",
    "\n",
    "    return subprocess.run(\n",
    "        cmd,\n",
    "        cwd=case_dir,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f0dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def parse_report(report_dir):\n",
    "    stats = dict(tests=0, failures=0, errors=0, skipped=0)\n",
    "\n",
    "    for xml_file in report_dir.glob(\"*.xml\"):\n",
    "        root = ET.parse(xml_file).getroot()\n",
    "\n",
    "        stats[\"tests\"] += int(root.attrib.get(\"tests\", 0))\n",
    "        stats[\"failures\"] += int(root.attrib.get(\"failures\", 0))\n",
    "        stats[\"errors\"] += int(root.attrib.get(\"errors\", 0))\n",
    "        stats[\"skipped\"] += int(root.attrib.get(\"skipped\", 0))\n",
    "\n",
    "    stats[\"passed\"] = (\n",
    "        stats[\"tests\"]\n",
    "        - stats[\"failures\"]\n",
    "        - stats[\"errors\"]\n",
    "        - stats[\"skipped\"]\n",
    "    )\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "PACKAGE_REGEX = re.compile(r'^\\s*package\\s+[\\w\\.]+;\\s*$', re.MULTILINE)\n",
    "\n",
    "def remove_package_declaration(java_file: Path):\n",
    "    text = java_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "    new_text, count = PACKAGE_REGEX.subn(\"\", text)\n",
    "\n",
    "    # delete all imports for import com.thealgorithms.* since we provide it locally\n",
    "    content = re.sub(r'^\\s*import\\s+com\\.thealgorithms\\.[\\w\\.]+;\\s*$', '', string=new_text, flags=re.MULTILINE)\n",
    "\n",
    "    java_file.write_text(content.lstrip(), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def process_directory(root: Path):\n",
    "    for java_file in root.rglob(\"*.java\"):\n",
    "        remove_package_declaration(java_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a14db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_public_methods(file_path):\n",
    "    # Regex Breakdown:\n",
    "    # 1. Look for 'public' methods\n",
    "    # 2. Match the return type (includes generics like List<String>)\n",
    "    # 3. Capture the method name (?P<method_name>...)\n",
    "    # 4. Ensure it is followed by '('\n",
    "    method_regex = r\"public\\s+(?:static\\s+)?[\\w<>[\\]]+\\s+(?P<method_name>\\w+)\\s*\\(\"\n",
    "    \n",
    "    method_names = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "            # finditer returns all non-overlapping matches\n",
    "            matches = re.finditer(method_regex, content)\n",
    "            for match in matches:\n",
    "                method_names.append(match.group(\"method_name\"))\n",
    "                \n",
    "        return method_names\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def overwrite_method_names_best_effort(target_file_path, new_names):\n",
    "    with open(target_file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # The regex matching the signature\n",
    "    pattern = r\"(public\\s+(?:static\\s+)?[\\w<>[\\]]+\\s+)(\\w+)(\\s*\\()\"\n",
    "    \n",
    "    # Find all matches to calculate our limits\n",
    "    matches = list(re.finditer(pattern, content))\n",
    "    num_found = len(matches)\n",
    "    num_provided = len(new_names)\n",
    "\n",
    "    if num_found != num_provided:\n",
    "        print(f\"Warning: Mismatch found. File has {num_found} methods, but {num_provided} names provided.\")\n",
    "        print(\"Proceeding with best-effort matching...\")\n",
    "\n",
    "    # We only iterate up to the smaller of the two lists\n",
    "    limit = min(num_found, num_provided)\n",
    "    \n",
    "    # We work backwards! Replacing from the end of the file to the start \n",
    "    # ensures that changing string lengths doesn't mess up our match offsets.\n",
    "    content_list = list(content)\n",
    "    \n",
    "    for i in range(limit - 1, -1, -1):\n",
    "        match = matches[i]\n",
    "        new_name = new_names[i]\n",
    "        \n",
    "        # match.start(2) and match.end(2) point exactly to the old method name\n",
    "        start, end = match.span(2)\n",
    "        content_list[start:end] = list(new_name)\n",
    "\n",
    "    updated_content = \"\".join(content_list)\n",
    "\n",
    "    with open(target_file_path, 'w') as f:\n",
    "        f.write(updated_content)\n",
    "\n",
    "    #print(f\"Successfully updated {limit} method(s) in {target_file_path}\")\n",
    "\n",
    "\n",
    "def overwrite_method_names(target_file_path, new_names):\n",
    "    with open(target_file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # This regex matches the same pattern as before:\n",
    "    # Group 1: Modifiers and return type\n",
    "    # Group 2: The old method name\n",
    "    # Group 3: The opening parenthesis and beyond\n",
    "    pattern = r\"(public\\s+(?:static\\s+)?[\\w<>[\\]]+\\s+)(\\w+)(\\s*\\()\"\n",
    "    \n",
    "    # Find all current public methods to check the count\n",
    "    current_methods = re.findall(pattern, content)\n",
    "    \n",
    "    if len(current_methods) != len(new_names):\n",
    "        raise ValueError(\n",
    "            f\"Count mismatch! File has {len(current_methods)} methods, \"\n",
    "            f\"but you provided {len(new_names)} names.\"\n",
    "        )\n",
    "\n",
    "    # Create a mapping of old method names to new names\n",
    "    old_method_names = [match[1] for match in current_methods]\n",
    "    method_name_mapping = dict(zip(old_method_names, new_names))\n",
    "\n",
    "    # First pass: rename method definitions\n",
    "    name_index = 0\n",
    "\n",
    "    def replacement_callback(match):\n",
    "        nonlocal name_index\n",
    "        result = f\"{match.group(1)}{new_names[name_index]}{match.group(3)}\"\n",
    "        name_index += 1\n",
    "        return result\n",
    "\n",
    "    updated_content = re.sub(pattern, replacement_callback, content)\n",
    "\n",
    "    # Second pass: rename method calls\n",
    "    for old_name, new_name in method_name_mapping.items():\n",
    "        # Match method calls: word boundary + method name + (\n",
    "        call_pattern = rf\"\\b{re.escape(old_name)}\\s*\\(\"\n",
    "        updated_content = re.sub(call_pattern, f\"{new_name}(\", updated_content)\n",
    "\n",
    "    with open(target_file_path, 'w') as f:\n",
    "        f.write(updated_content)\n",
    "\n",
    "    #print(f\"Successfully updated {len(new_names)} methods in {target_file_path}\")\n",
    "\n",
    "\n",
    "def rename_class_in_file(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    new_class_name = file_name.replace(\".java\", \"\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # remove comments to avoid false positives\n",
    "    content_no_comments = re.sub(r\"//.*?$|/\\*.*?\\*/\", \"\", content, flags=re.DOTALL | re.MULTILINE)\n",
    "\n",
    "    # 1. Identify the OLD class name first so we know what constructor to look for\n",
    "    # We look for the current class name after the 'class' keyword\n",
    "    old_name_match = re.search(r\"class\\s+([a-zA-Z0-9_]+)\", content_no_comments)\n",
    "    if not old_name_match:\n",
    "        return\n",
    "    old_class_name = old_name_match.group(1)\n",
    "\n",
    "    # 2. Rename the Class Definition\n",
    "    class_pattern = r\"((?:(?:public|final|protected|private|abstract)\\s+)*class\\s+)[a-zA-Z0-9_]+\"\n",
    "    content_no_comments = re.sub(class_pattern, rf\"\\1{new_class_name}\", content_no_comments, count=1)\n",
    "\n",
    "    # 3. Rename the Constructor(s)\n",
    "    # Pattern: Modifier (optional) + Old Name + (\n",
    "    # We use a lookahead to ensure it's a constructor and not a method call\n",
    "    constr_pattern = rf\"((?:public|protected|private)\\s+){old_class_name}\\s*\\(\"\n",
    "    content_no_comments = re.sub(constr_pattern, rf\"\\1{new_class_name}(\", content_no_comments)\n",
    "\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(content_no_comments)\n",
    "    \n",
    "    #print(f\"Removed comments and updated class and constructor in: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6145c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "ORIGINAL_DIR = Path(\"../input/testplayground/original\")\n",
    "TESTS_DIR = Path(\"../input/testplayground/tests\")\n",
    "DATASET_DIR = Path(\"../input/testplayground/dataset\")\n",
    "SUPPORT_DIR = Path(\"../input/testplayground/support\")\n",
    "\n",
    "# Matches: ClassName_KF_Version_Prompt.java\n",
    "IMPL_PATTERN = re.compile(r\"^(?P<base>[^_]+)_(?P<KeyFactor>[^_]+)_(?P<Version>[^_]+)_(?P<Prompt>[^_]+)\\.java$\")\n",
    "\n",
    "case_class_function_name_mapping = []\n",
    "\n",
    "def build_dataset():\n",
    "    for impl_file in ORIGINAL_DIR.rglob(\"*.java\"):\n",
    "        match = IMPL_PATTERN.match(impl_file.name)\n",
    "        if not match:\n",
    "            print(f\"Skipping {impl_file.name} (does not match pattern)\")\n",
    "            continue\n",
    "\n",
    "        base = match.group(\"base\")\n",
    "        key_factor = match.group(\"KeyFactor\")\n",
    "        version = match.group(\"Version\")\n",
    "        prompt = match.group(\"Prompt\")\n",
    "\n",
    "        test_file = TESTS_DIR / f\"{base}Test.java\"\n",
    "        if not test_file.exists():\n",
    "            print(f\"⚠ No test found for {base}, skipping\")\n",
    "            continue\n",
    "\n",
    "        if version == \"v2\" or version == \"v3\" or version == \"v4\" or version == \"v5\":\n",
    "            # todo remove this exception when everything runs\n",
    "            #print(f\"⚠ For speed, we are currently skipping later iterations\")\n",
    "            #continue\n",
    "            pass\n",
    "\n",
    "        case_dir = DATASET_DIR / \"new\" / base / key_factor / version / prompt\n",
    "        case_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Copy + rename implementation\n",
    "        shutil.copyfile(\n",
    "            impl_file,\n",
    "            case_dir / f\"{base}.java\"\n",
    "        )\n",
    "\n",
    "        # copy over supporting files\n",
    "        def copy_support_files(case_dir):\n",
    "            for support_file in SUPPORT_DIR.glob(\"*.java\"):\n",
    "                shutil.copyfile(\n",
    "                    support_file,\n",
    "                    case_dir / support_file.name\n",
    "                )\n",
    "\n",
    "        copy_support_files(case_dir)\n",
    "\n",
    "        # Copy + rename class name in implementation to match file name\n",
    "        rename_class_in_file(case_dir / f\"{base}.java\")\n",
    "\n",
    "        # Extract public static method names and store mapping\n",
    "        if key_factor == \"KF0\" and version == \"v0\" and prompt == \"nop\":\n",
    "            method_names = extract_public_methods(case_dir / f\"{base}.java\")\n",
    "            case_class_function_name_mapping.append({\n",
    "                \"class_name\": base,\n",
    "                \"method_names\": method_names\n",
    "            })\n",
    "        else:\n",
    "            if not any(mapping[\"class_name\"] == base for mapping in case_class_function_name_mapping):\n",
    "                print(f\"missing mapping for {base}/{key_factor}/{version}/{prompt}\")\n",
    "            else:\n",
    "                #print(f\"Found mapping for {base}/{key_factor}/{version}/{prompt}\")\n",
    "                \n",
    "                # If we have a mapping, overwrite method names in this implementation\n",
    "                # We assume the order of methods in the file is consistent across versions, so we can reuse the same method names.\n",
    "                # get the method names for this class\n",
    "                mapping = next((m for m in case_class_function_name_mapping if m[\"class_name\"] == base), None)\n",
    "\n",
    "                try:\n",
    "                    overwrite_method_names(target_file_path=case_dir / f\"{base}.java\", new_names=mapping[\"method_names\"])\n",
    "                except ValueError as e:\n",
    "                    print(f\"Error: {e}: Trying best-effort alternative\")\n",
    "                    try:\n",
    "                        overwrite_method_names_best_effort(target_file_path=case_dir / f\"{base}.java\", new_names=mapping[\"method_names\"])\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Error: {e}: Best-effort alternative also failed\")\n",
    "\n",
    "\n",
    "        # Copy test file\n",
    "        shutil.copyfile(\n",
    "            test_file,\n",
    "            case_dir / test_file.name\n",
    "        )\n",
    "\n",
    "        #print(f\"Created case: {base}/{key_factor}/{version}/{prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b560c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DIRECTORY = Path(\"../input/testplayground/\")\n",
    "DATASET = Path.joinpath(DIRECTORY, \"dataset\")\n",
    "\n",
    "build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import tqdm\n",
    "\n",
    "LIB_DIR = Path(\"lib\")\n",
    "classpath = CP_SEP.join(str(Path.joinpath(DIRECTORY, jar)) for jar in LIB_DIR.glob(\"*.jar\"))\n",
    "\n",
    "#print(f\"Using classpath: {classpath}\")\n",
    "\n",
    "# grab a specific list of cases to run\n",
    "directory_to_run = DATASET.joinpath(\"failing\", \"new\")\n",
    "\n",
    "# get all folder names in directory_to_run\n",
    "folder_names = [\n",
    "    folder.name\n",
    "    for folder in directory_to_run.iterdir()\n",
    "    if folder.is_dir()\n",
    "]\n",
    "\n",
    "folder_names = [\"JugglerSequence\"]\n",
    "\n",
    "case_test_results = []\n",
    "\n",
    "# Pre-scan case directories so we know total progress\n",
    "case_dirs = [\n",
    "    case\n",
    "    for case in DATASET.joinpath(\"new\").rglob(\"*\")\n",
    "    if case.is_dir() and any(case.glob(\"*.java\"))\n",
    "]\n",
    "\n",
    "for case in tqdm.tqdm(case_dirs, total=len(case_dirs), desc=\"Compiling and running test cases\"):\n",
    "    current_case = case.relative_to(DATASET.joinpath(\"new\")).parts[0]\n",
    "    #print(f\"folder of interest {current_case}\")\n",
    "    #print(f\"acceptable folders {folder_names}\")\n",
    "    if current_case not in folder_names:\n",
    "        print(f\"Skipping {current_case} since it's not in the specified list of folders to run\")\n",
    "        continue\n",
    "\n",
    "    compile_res = compile_case(case, classpath)\n",
    "\n",
    "    # get relative path from DATASET root\n",
    "    path = case.relative_to(DATASET.joinpath(\"new\"))\n",
    "    \n",
    "    if compile_res.returncode != 0:\n",
    "        case_test_results.append({\n",
    "            \"case\": str(path),\n",
    "            \"status\": \"compile_error\",\n",
    "            \"error\": compile_res.stderr\n",
    "        })\n",
    "\n",
    "        # move failing compiles into the \"failing\" directory for easier access\n",
    "        failing_dir = DATASET.joinpath(\"failing\", path.parent)\n",
    "        failing_dir.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.move(str(case), str(failing_dir / case.name))\n",
    "\n",
    "        continue\n",
    "\n",
    "    test_result = run_tests(case, classpath)\n",
    "    #print(test_result)\n",
    "\n",
    "    stats = parse_report(case)\n",
    "    stats[\"case\"] = str(path)\n",
    "    stats[\"status\"] = \"ok\"\n",
    "    case_test_results.append(stats)\n",
    "\n",
    "    if False:\n",
    "        if stats[\"failures\"] > 0 or stats[\"errors\"] > 0:\n",
    "            # move failing cases into the \"failing\" directory for easier access\n",
    "            failing_dir = DATASET.joinpath(\"failing\", path.parent)\n",
    "            failing_dir.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(case, failing_dir / case.name)\n",
    "        else:\n",
    "            # move passing cases into the \"passing\" directory for easier access\n",
    "            passing_dir = DATASET.joinpath(\"passing\", path.parent)\n",
    "            passing_dir.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.move(case, passing_dir / case.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8930fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the number of passed and failed tests\n",
    "passed_tests = sum(1 for result in case_test_results if result[\"status\"] == \"ok\" and result.get(\"passed\", 0) > 0)\n",
    "failed_tests = sum(1 for result in case_test_results if result[\"status\"] == \"ok\" and result.get(\"failed\", 0) > 0)\n",
    "compile_errors = sum(1 for result in case_test_results if result[\"status\"] == \"compile_error\")\n",
    "\n",
    "print(f\"Total cases with passed tests: {passed_tests}\")\n",
    "print(f\"Total cases with failed tests: {failed_tests}\")\n",
    "print(f\"Total cases with compile errors: {compile_errors}\")\n",
    "\n",
    "# print cases where there is a compile error\n",
    "for result in case_test_results:\n",
    "    if result[\"status\"] == \"compile_error\":\n",
    "        print(f\"Compile error in case {result['case']}:\")\n",
    "        print(result[\"error\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c849bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go into each case and parse the report\n",
    "dir = DATASET.joinpath(\"new\")\n",
    "repos = []\n",
    "for case in dir.rglob(\"*\"):\n",
    "    if case.is_dir() and any(case.glob(\"*.java\")):\n",
    "        print(f\"Processing case: {case.relative_to(DATASET)}\")\n",
    "        \n",
    "        # read xml report\n",
    "        report_stats = parse_report(case)\n",
    "\n",
    "        print(report_stats)\n",
    "\n",
    "        print(f\"Total cases with passed tests: {report_stats['passed']}\")\n",
    "        print(f\"Total cases with failed tests: {report_stats['failures'] + report_stats['errors']}\")\n",
    "\n",
    "        if report_stats[\"failures\"] > 0 or report_stats[\"errors\"] > 0:\n",
    "            # move failing cases into the \"failing\" directory for easier access\n",
    "            failing_dir = DATASET.joinpath(\"failing\", case.relative_to(DATASET))\n",
    "            failing_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            print(f\"Moving failing case {case.relative_to(DATASET)} from {case} to {failing_dir / case.name}\")\n",
    "\n",
    "            shutil.move(case, failing_dir / case.name)\n",
    "        else:\n",
    "            # move passing cases into the \"passing\" directory for easier access\n",
    "            passing_dir = DATASET.joinpath(\"passing\", case.relative_to(DATASET))\n",
    "            passing_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            print(f\"Moving passing case {case.relative_to(DATASET)} from {case} to {passing_dir / case.name}\")\n",
    "\n",
    "            shutil.move(case, passing_dir / case.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2718e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataframe with all snippets, variants, and versions\n",
    "\n",
    "all_code_path = Path(\"../input/testplayground/all_code\")\n",
    "\n",
    "all_codes = []\n",
    "\n",
    "for case in all_code_path.rglob(\"*\"):\n",
    "    if case.is_dir() and any(case.glob(\"*.java\")):\n",
    "        snippet_name = case.name\n",
    "        for impl_file in case.glob(\"*.java\"):\n",
    "            match = IMPL_PATTERN.match(impl_file.name)\n",
    "            if not match:\n",
    "                print(f\"Skipping {impl_file.name} (does not match pattern)\")\n",
    "                continue\n",
    "\n",
    "            base = match.group(\"base\")\n",
    "            key_factor = match.group(\"KeyFactor\")\n",
    "            version = match.group(\"Version\")\n",
    "            prompt = match.group(\"Prompt\")\n",
    "\n",
    "            code = impl_file.read_text(encoding=\"utf-8\")\n",
    "\n",
    "            all_codes.append({\n",
    "                \"snippet\": snippet_name,\n",
    "                #\"class_name\": base,\n",
    "                \"key_factor\": key_factor,\n",
    "                \"version\": version,\n",
    "                \"prompt\": prompt\n",
    "            })\n",
    "\n",
    "import pandas as pd\n",
    "df_test_results = pd.DataFrame(all_codes)\n",
    "\n",
    "# print dimensions\n",
    "print(f\"All code dataframe dimensions: {df_test_results.shape}\")\n",
    "print(df_test_results.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3a32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "currently_passing = DATASET.joinpath(\"passing\", \"new\")\n",
    "currently_failing = DATASET.joinpath(\"failing\", \"new\")\n",
    "\n",
    "def evaluate_test_results(folder_to_search):\n",
    "    snippets = 0\n",
    "    tests_passed = 0\n",
    "    tests_failed = 0\n",
    "\n",
    "    for case in folder_to_search.rglob(\"*\"):\n",
    "        if case.is_dir() and any(case.glob(\"*.java\")):\n",
    "            #print(f\"Processing case: {case.relative_to(DATASET)}\")\n",
    "            snippets += 1\n",
    "\n",
    "            # extract KF and version from path\n",
    "            path_parts = case.relative_to(DATASET).parts\n",
    "        \n",
    "            snippet = path_parts[-5]\n",
    "            key_factor = path_parts[-4]\n",
    "            version = path_parts[-3]\n",
    "            prompt = path_parts[-2]\n",
    "            #print(f\"Key factor: {key_factor}, version: {version}, prompt: {prompt}\")\n",
    "\n",
    "            # read xml report\n",
    "            report_stats = parse_report(case)\n",
    "\n",
    "            #print(report_stats)\n",
    "\n",
    "            if report_stats[\"failures\"] > 0 or report_stats[\"errors\"] > 0:\n",
    "                tests_failed += report_stats[\"failures\"]\n",
    "                tests_failed += report_stats[\"errors\"]\n",
    "            else:\n",
    "                tests_passed += report_stats[\"passed\"]\n",
    "\n",
    "            # add results to dataframe where snippet, key_factor, version, and prompt match\n",
    "            mask = (df_test_results[\"snippet\"] == snippet) & (df_test_results[\"key_factor\"] == key_factor) & (df_test_results[\"version\"] == version) & (df_test_results[\"prompt\"] == prompt)\n",
    "            df_test_results.loc[mask, \"tests_passed\"] = report_stats[\"passed\"]\n",
    "            df_test_results.loc[mask, \"tests_failed\"] = report_stats[\"failures\"] \n",
    "            df_test_results.loc[mask, \"tests_errors\"] = report_stats[\"errors\"]\n",
    "        \n",
    "\n",
    "    print(f\"Total snippets: {snippets}\")\n",
    "    print(f\"Total tests passed: {tests_passed}\")\n",
    "    print(f\"Total tests failed: {tests_failed}\")\n",
    "\n",
    "evaluate_test_results(currently_passing)\n",
    "evaluate_test_results(folder_to_search=currently_failing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test_results.to_csv(DIRECTORY / \"all_code_test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e526d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_class_in_file_to_filename(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    new_class_name = file_name.replace(\".java\", \"\")\n",
    "    \n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # 1. Identify the OLD class name first so we know what constructor to look for\n",
    "    # We look for the current class name after the 'class' keyword\n",
    "    old_name_match = re.search(r\"class\\s+([a-zA-Z0-9_]+)\", content)\n",
    "    if not old_name_match:\n",
    "        return\n",
    "    old_class_name = old_name_match.group(1)\n",
    "\n",
    "    # 2. Rename the Class Definition\n",
    "    class_pattern = r\"((?:(?:public|final|protected|private|abstract)\\s+)*class\\s+)[a-zA-Z0-9_]+\"\n",
    "    content = re.sub(class_pattern, rf\"\\1{new_class_name}\", content, count=1)\n",
    "\n",
    "    # 3. Rename the Constructor(s)\n",
    "    # Pattern: Modifier (optional) + Old Name + (\n",
    "    # We use a lookahead to ensure it's a constructor and not a method call\n",
    "    constr_pattern = rf\"((?:public|protected|private)\\s+){old_class_name}\\s*\\(\"\n",
    "    content = re.sub(constr_pattern, rf\"\\1{new_class_name}(\", content)\n",
    "\n",
    "    with open(file_path, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# help to prepare a manual inspection of test case by collecting all versions\n",
    "dir_test_project = Path(\"../TestProject/\")\n",
    "input_string = \"ADFGVXCipher\"\n",
    "\n",
    "key_factor = [\"KF0\", \"KF1\", \"KF2\"]\n",
    "versions = [\"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\"]\n",
    "prompts = [\"nop\", \"pKF0\", \"pKF1\", \"pKF2\"]\n",
    "\n",
    "test_file_done = False\n",
    "\n",
    "for kf in key_factor:\n",
    "    for version in versions:\n",
    "        for prompt in prompts:\n",
    "            case_dir = DATASET.joinpath(\"failing\", \"new\", input_string, kf, version, prompt)\n",
    "            if case_dir.exists():\n",
    "                print(f\"Case: {input_string}_{kf}_{version}_{prompt}\")\n",
    "                \n",
    "                # get the test file\n",
    "                test_file = case_dir / prompt / f\"{input_string}Test.java\"\n",
    "                if test_file.exists():\n",
    "                    if not test_file_done:\n",
    "                        target_test_file = dir_test_project / f\"test/{input_string}Test.java\"\n",
    "                        shutil.copyfile(test_file, target_test_file)\n",
    "                        print(f\"Copied {test_file} to {target_test_file}\")\n",
    "                        test_file_done = True\n",
    "                else:\n",
    "                    print(f\"Test file not found for {input_string}/{kf}/{version}/{prompt}/{prompt}\")\n",
    "\n",
    "                # get the Java file with the input_string name\n",
    "                java_file = case_dir / prompt / f\"{input_string}.java\"\n",
    "                #print(f\"Looking for Java file at {java_file}\")\n",
    "                if java_file.exists():\n",
    "                    # rename the file to match input_string\n",
    "                    target_file = dir_test_project / f\"src/{input_string}_{kf}_{version}_{prompt}.java\"\n",
    "                    shutil.copyfile(java_file, target_file)\n",
    "                    rename_class_in_file_to_filename(target_file)\n",
    "                    print(f\"Copied and adjusted {java_file} to {target_file}\")\n",
    "                else:\n",
    "                    print(f\"File not found for {input_string}/{kf}/{version}/{prompt}/{prompt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .csv file again for further analysis\n",
    "df_results = pd.read_csv(DIRECTORY / \"all_code_test_results.csv\")\n",
    "df_results.head(5)\n",
    "\n",
    "# number of unique snippets that had tests run (i.e. tests_passed + tests_failed > 0)\n",
    "df_results[\"total_tests\"] = df_results[\"tests_passed\"] + df_results[\"tests_failed\"] + df_results[\"tests_errors\"]\n",
    "num_snippets_with_tests = df_results[df_results[\"total_tests\"] > 0][\"snippet\"].nunique()\n",
    "print(f\"Number of unique snippets with tests run: {num_snippets_with_tests}\")\n",
    "\n",
    "# if the values in the column \"man_tests_passed\" are empty, fill them with the values from \"tests_passed\"\n",
    "if \"man_tests_passed\" in df_results.columns and df_results[\"man_tests_passed\"].isnull().all():\n",
    "    df_results[\"man_tests_passed\"] = df_results[\"tests_passed\"]\n",
    "    df_results[\"man_tests_failed\"] = df_results[\"tests_failed\"]\n",
    "    df_results[\"man_tests_errors\"] = df_results[\"tests_errors\"]\n",
    "\n",
    "# run some analysis\n",
    "# first: show total number of tests passed and failed\n",
    "total_passed = df_results[\"tests_passed\"].sum()\n",
    "total_failed = df_results[\"tests_failed\"].sum() + df_results[\"tests_errors\"].sum()\n",
    "print(f\"Total tests passed: {total_passed}\")\n",
    "print(f\"Total tests failed: {total_failed}\")\n",
    "\n",
    "# first: compare tests_failed for all prompt versions (nop, pKF0, pKF1, pKF2)\n",
    "# print summary statistics in percentage\n",
    "for version in versions:\n",
    "    for kf in key_factor:\n",
    "        subset = df_results[(df_results[\"version\"] == version) & (df_results[\"key_factor\"] == kf)]\n",
    "        total_cases = len(subset)\n",
    "        failed_cases = subset[\"tests_failed\"].sum() + subset[\"tests_errors\"].sum()\n",
    "        passed_cases = subset[\"tests_passed\"].sum()\n",
    "        \n",
    "        print(f\"Version: {version}, Key Factor: {kf}\")\n",
    "        print(f\"Total cases: {total_cases}\")\n",
    "        print(f\"Passed cases: {passed_cases} ({passed_cases / (passed_cases + failed_cases) * 100:.2f}%)\")\n",
    "        print(f\"Failed cases: {failed_cases} ({failed_cases / (passed_cases + failed_cases) * 100:.2f}%)\")\n",
    "        print(\"-\" * 30)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
