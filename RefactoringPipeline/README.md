# LLM-DiffEx (Large Language Model Diff Explorer)
---
This tool was developed to analyze systematically and fully automated the code evolution of iterative, subsequent LLM refactorings under different prompting strategies.

Currently, only Java code is supported to be analyzed.

The LLM used is ChatGPT, but adapting the API calls to another LLM is fairly little change.

---
## Project Structure and Execution Overview

Before running the LLM-DiffEx pipeline for the first time, it is important to understand which folders will be created, which data is reused, and **how configuration parameters influence the behavior of the system**.

The project uses two main top-level directories:

`input/`

`output/`

Both contain **automatically generated subfolders**, named after the folder you chose to analyze (the “project name”).

---
#### `input/<project_name>/`

This folder contains *all files that belong to the dataset*, including:

- The original Java files you want to analyze
- The variants you created during the Dataset Creation Process (optional. If you don't provide any of the implemented variants, the dataset only contains the original code, aka "base variant")
- Files generated by the LLM API during the dataset creation process (Note: You need a valid API key for that!)
- Metadata files created by the FileProcessor.

Automatically generated metadata includes:

- `original_dataset.json` — metrics and conditions of the unmodified codebase
- `generated_dataset.json` — complete dataset including all variants and LLM-generated versions

---
#### `output/<project_name>/<LOC-range>/`

This folder stores all **comparison results**, **analysis summaries**, and **diff artifacts** associated with the specific LOC interval you configured.  

Contents include:

- Summary statistics
- LineDiff comparison data
- Individual snippet folders containing:
  - raw diff output
  - parsed diff structures
  - analysis metrics

This ensures that experiments using different LOC ranges never overwrite each other.

---

## Usage
---
### 1. To install all required Python packages, run this little Bash script:

```
#!/bin/bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```
---
### 2. Create your API key and save it to an environment variable (s. description below).

To have code generated via ChatGPT, you need a valid **OpenAI API key**.  

Follow these steps to create one:

1. **Log in to OpenAI**
   Visit the OpenAI platform at  
   https://platform.openai.com/  
   and log in with your account.

2. **Navigate to: API Keys**
   In the left sidebar, go to:  
   **User Settings → API Keys**

3. **Create a New Secret Key**
   Click **“Create new secret key”** and give it a clear name (e.g., `LLM-DiffEx`).

4. **Copy the Key Immediately**
   This is the **only time** you will see the full key.  
   Copy it and store it in a safe place (e.g., in your password manager).

5. **Set the Key as an Environment Variable**
   For example, on macOS or Linux (bash/zsh):

   ```bash
   export OPENAI_API_KEY="your-secret-key-here"

**Important**:
- Keep your API key private.
- Do not commit it to Git or store it inside your project folder.
- Make sure your environment variables are loaded when running scripts (via CLI or IDE).

---
### 3. Decide which Java files you want to analyze and place them in a folder anywhere (<folder name>).

You provide the pipeline (more precisely, the `FileProcessor`) with a folder containing the Java files to be analyzed. (All other types of files will be ignored.)

The FileProcessor examines each file based on a set of **structural and absolute metrics**. The extracted information is stored in a JSON file in the project's `/input/<project_name>/original_dataset.json` with the following fields:

* **algorithm name**: The base name of the Java file.
* **variant**: Always `config.base_variant` for the initial analysis, representing the *original code* without any transformations.
* **version**: Always `config.base_version` for the base version.
* **prompt_id**: Set to `config.no_prompt`, indicating that these files were not generated by a prompt, unlike later versions.
* **metrics**: A collection of structural measurements, including the full code of the file and several heuristically derived conditions.

#### Metrics and Conditions

Among the collected metrics are two previously defined heuristic conditions:

##### **Code > Comment**

Indicates whether the file contains more lines of code than lines of comments.

##### **Method Ratio**

A heuristic to assess whether the number of methods in a file is reasonable given its size.

Rules used by the method-ratio heuristic (see `utils.py` for implementation):

* Files with **fewer than 20 lines** should have **at most 1 method**.
* Files with **fewer than 50 lines** should have **1 to 3 methods**.
* Larger files receive a dynamic acceptable method range based on their total line count (LOC).

#### Example JSON Output:

```json
{
    "A5Cipher": {
        "KF0": {
            "v0": {
                "nop": {
                    "Code": "*the actual code that the file contains*",
                    "Code > Comment": false,
                    "Code Lines": 20,
                    "Comment Lines": 35,
                    "Empty Lines": 8,
                    "Inline Comments": 1,
                    "Method Ratio": true,
                    "Methods": 2,
                    "Total Lines": 63
                }
            }
        }
    }
}
```
---
### 4. Set LOC interval constants and llm_iterations in config/constants.py.

LOC interval settings are defined in:

`config/constants.py`

These parameters determine:

- which files (based on size and the other predefined conditions) enter the dataset,
- how they are grouped into LOC intervals,
- how the output directory is labeled.

#### Two Use-Cases for LOC Interval Settings

**1. Exploring an Unknown Codebase**

Goal: Identify which LOC ranges contain the most suitable files (based on structural snippet criteria such as method ratio and Code > Comment).
Recommended configuration:

```
min_LOC = 0
max_LOC = 1_000_000
interval_size = <desired interval size>
llm_iterations = 0 # this makes sure that even when you accidentally run the dataset creation step, no API calls are made (= no unintentional costs, yay!)
```

The resulting Summary Report (in `input/<folder name>/`<LOC interval>_Summary_Counts.csv/.md` shows:
- how many files fall into each interval,
- how many satisfy the snippet selection criteria.

Example for the `LOC_0-1000000_Summary_Counts.md` file of the thesis dataset (https://github.com/TheAlgorithms/Java/tree/master):

| LOC Interval   |   Total_Files |   Method_Ratio |   Code_Comment_Ratio |   Valid_Files |
|:---------------|--------------:|---------------:|---------------------:|--------------:|
| LOC_0-49       |           209 |            202 |                  150 |           143 |
| LOC_50-99      |           287 |            227 |                  187 |           148 |
| LOC_100-149    |            75 |             56 |                   60 |            45 |
| LOC_150-199    |            45 |             32 |                   38 |            28 |
| LOC_200-249    |            18 |              9 |                   17 |             9 |
| LOC_250-299    |             9 |              4 |                    8 |             3 |
| LOC_300-349    |             8 |              8 |                    8 |             8 |
| LOC_350-399    |             1 |              0 |                    1 |             0 |
| LOC_400-449    |             1 |              1 |                    1 |             1 |
| LOC_450-499    |             2 |              0 |                    2 |             0 |
| LOC_500-549    |             1 |              0 |                    1 |             0 |
| LOC_1200-1249  |             1 |              0 |                    1 |             0 |
| LOC_2750-2799  |             1 |              0 |                    1 |             0 |

This helps determine the ideal LOC range for subsequent experiments.

**2. Running a focused experiment on a known interval**

After identifying a meaningful LOC range (e.g., 50–200 LOC), set:

```
min_LOC = 50
max_LOC = 200
interval_size = <some value between min_LOC and max_LOC>   # used for dataset key "LOC_label"
llm_iterations = 5 # how many refactoring iteration steps the LLMs should take
```
A smaller interval_size than the complete size of the interval borders (i.e., 150 in that example case here) is allowed and could be used later for sub-grouping, though the current pipeline does not yet use the LOC_label for downstream grouping.

The chosen interval borders automatically become part of the output directory name:

`output/<project_name>/LOC_50-200`

---
### 5. Run the scripts:

**Option A:** 

You run everything for the first time and need to find out what the best values for the LOC interval constants should be given your dataset.

→ In that case, run the FileProcessor separately first with the LOC interval constants as described before and skip the dataset creation (say "n" (no) when you get asked via console).

**Option B:** 

You have everything set up and want the whole dataset to be generated - or the already generated one used - and analyzed + evaluated.

`python -m src.main <folder name>`

This runs the whole pipeline. (!! => only useful once you already anaylzed the base files and know the final values for LOC interval constants and llm_iterations.)

**Option C:**

You have parts of the dataset generated or parts of the comparison analysis, or e.g. want to generate some new visualizations on some data...there are plenty of use cases in which you only want to run parts of the pipeline.

=> To run each major script/step individually:

---
`python -m src.FileProcessor <folder name>`

→ creates `input/<folder name>` in LLM-DiffEx, where all dataset files will live (as single .java files as well as one cummulative .json file per dataset).
→ creates corresponding folder in `output/<folder name>/<LOC interval>` where all the comparison and analysis data will live

**Result:**

in `input/<folder name>`:
- `original_dataset.json` = data and metrics of the original Java files only
- `<LOC interval>_Summary_Counts.csv/.md` = quick overview how many files fulfill all our criteria in which LOC interval
- `<LOC interval>_filtered_dataset.json` = subset of original_dataset.json with only files that fulfill our criteria 
- `<LOC interval>_generated_dataset.json` = complete dataset with all variants and versions of the filtered dataset (after all LLM iterations are finished)
- `snippets/` => all original and generated variants and versions as .java files per algorithm (in case we want to look at some of them by hand it’s nice to have them not only inside a huge JSON file)

in `output/<folder name>/<LOC interval>`:
- `Absolute_Values.csv/.md` = Structural Metrics of the dataset in that LOC interval (Snippet,Variant,Version,Prompt,Total Lines,Code Lines,Comment Lines,Inline Comments,Empty Lines,Methods,Code > Comment,Method Ratio,LOC Interval)
  
---
`python -m src.CodeComparer /input/<folder name>`

→ takes the _generated_dataset.json
→ calculates the diffs for all eligible snippets. This includes:
    *LineDiff object creation for all diffs
    *Parsing of the diffs, i.e. classification of insertions, modifications, deletions and other metrics
    *Constructing modified diffs with classification comments and optional info on matches/ratio/change type data
    *Constructing (sub-)graphs of the code evolution in each step and at the end for each code evolution line (defined by variant and prompt)

**Results:**

in `output/<folder name>/<LOC interval>`:

- `Comparison_Report.csv/.md`
- `Change_Types_Report.csv/.md`
- `snippets/` => all comparison data per algorithm, i.e. 
    - line diffs, 
    - line diffs with details on the classification on each line („LineDiff_modified“), 
    - logs, 
    - json files that hold all data of one diff object (such that expensive diff recalculation can be skipped if that has already been done before at some point)
    - code subgraphs per iteration
    - code graph per variant-prompt line

---
`python -m src.Evaluator output/<folder name>/<LOC interval>`
→ runs evaluation + visualization pipeline based on the files ``Absolute_Values.csv`, `Comparison_Report.csv`, and `Change_Types_Report.csv`.

**Results:**

in `output/<folder name>/<LOC interval>\analysis`:

- // TODO

in `output/<folder name>/<LOC interval>\plots`:

- // TODO


-------

## Dataset Reuse and Caching Behavior

The pipeline is optimized to **avoid unnecessary recomputation**, which saves time and reduces API usage. Several steps in the workflow automatically load previously created data instead of regenerating it.

### FileProcessor Caching

The FileProcessor uses a simple caching logic:

| File | Reused When | Regenerated When |
|------|-------------|------------------|
| `original_dataset.json` | Always reused if present | Only generated when missing |
| `generated_dataset.json` | Always reused if present | Only generated when missing |

This design makes it easy to pause dataset creation and continue later.  
If the LLM has already produced some variants, they are **not generated again**.


### When You Want a Fresh Dataset

If you want to run a completely new experiment, you should:

- rename the existing dataset, **or**
- move it into another folder (e.g., with a timestamp), **or**
- delete it manually.

A future quality-of-life feature could be automatic timestamping of LLM-generated datasets to track multiple experimental runs while keeping them separate.


#### Practical Implications

- You can stop the pipeline mid-run.
- On the next run, existing files are loaded automatically.
- Only still-missing LLM-generated versions are produced.
- This ensures consistency and reduces unnecessary API calls.

---

### CodeComparer Caching

The CodeComparer stores extensive comparison metadata to allow fast repeated runs.

All comparison-relevant data for each snippet combination is saved to:
```output/<project_name>/<LOC-range>/snippets/<algorithm_name>/data/```

When these files exist, the CodeComparer:

- **loads all diff objects from disk**
- **skips diff recalculation entirely**

This is especially helpful because diffing dozens of large files with multiple LLM variants can be computationally expensive.

#### Practical Benefits

- You can safely modify later pipeline stages (e.g., aggregation or visualization).
- A re-run does **not** redo all comparisons.
- Only missing or invalidated snippets are recomputed.

---


---


