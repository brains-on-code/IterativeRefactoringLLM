{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a2cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "GITHUB_TOKEN = \"SECRET-TODO\" # Replace with your GitHub token\n",
    "\n",
    "def get_new_java_repos(created_after=\"2025-01-01\"):\n",
    "    api_url = \"https://api.github.com/search/repositories\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"q\": f\"language:Java created:>{created_after}\",\n",
    "        \"sort\": \"created\",\n",
    "        \"order\": \"desc\",\n",
    "        \"per_page\": 10  # Max allowed per page\n",
    "    }\n",
    "\n",
    "    repositories = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        params[\"page\"] = page\n",
    "        response = requests.get(api_url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get(\"items\", [])\n",
    "            if not items:\n",
    "                break\n",
    "            \n",
    "            repositories.extend(items)\n",
    "            print(f\"Page {page}: Found {len(items)} repos...\")\n",
    "            \n",
    "            # Stop if we hit GitHub's 1000-result limit\n",
    "            if len(repositories) >= 20:\n",
    "                print(\"Reached result limit. Use date-slicing for more.\")\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            time.sleep(2) # Respect rate limits\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "            break\n",
    "            \n",
    "    return repositories\n",
    "\n",
    "repos = get_new_java_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df46163",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae17ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "import base64\n",
    "import time\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Set seed for consistent results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "CREATED_AFTER = \"2025-01-01\"  # Ensures zero training leakage\n",
    "SAVE_DIR = \"../input/GitHubScrape/\"\n",
    "MIN_LOC = 50\n",
    "MAX_LOC = 250\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"Isolates comments and checks if they are English.\"\"\"\n",
    "    # Extract text from /* ... */\n",
    "    multi_line = re.findall(r'/\\*([\\s\\S]*?)\\*/', text)\n",
    "    # Extract text from // ...\n",
    "    inline = re.findall(r'//(.*)', text)\n",
    "    \n",
    "    # Combine all comment text into one string\n",
    "    all_comments = \" \".join(multi_line + inline).strip()\n",
    "    \n",
    "    # Clean up common Java symbols/artifacts so they don't confuse the detector\n",
    "    clean_comments = re.sub(r'[*@\\n\\r\\t]', ' ', all_comments)\n",
    "    \n",
    "    # We need a decent amount of text to detect language accurately\n",
    "    if len(clean_comments) < 20: \n",
    "        return False\n",
    "        \n",
    "    try:\n",
    "        return detect(clean_comments) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def filter_java_content(content):\n",
    "    \"\"\"Checks for LOC, multi-line (/* */), and inline (//) comments.\"\"\"\n",
    "    lines = content.splitlines()\n",
    "    loc = len(lines)\n",
    "    \n",
    "    if not (MIN_LOC <= loc <= MAX_LOC):\n",
    "        return False\n",
    "    \n",
    "    # Regex for multi-line: /* ... */\n",
    "    has_multi = re.search(r'/\\*[\\s\\S]*?\\*/', content)\n",
    "\n",
    "    # Regex for inline: // ...\n",
    "    has_inline = re.search(r'//.*', content)\n",
    "\n",
    "    if not (has_multi and has_inline):\n",
    "        return False\n",
    "\n",
    "    # Finally, ensure those comments are English\n",
    "    return is_english(content)\n",
    "\n",
    "\n",
    "def download_java_files():\n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.makedirs(SAVE_DIR)\n",
    "\n",
    "    # 1. Search for Java repos created in 2025\n",
    "    search_url = f\"https://api.github.com/search/repositories?q=language:Java+created:>{CREATED_AFTER}&sort=stars\"\n",
    "    repos = requests.get(search_url, headers=HEADERS).json().get('items', [])\n",
    "    saved_files_overall = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bcbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "LICENSE = \"mit\" # Options: mit, apache-2.0, gpl-3.0, bsd-3-clause\n",
    "\n",
    "def download_bulk_java():\n",
    "    if not os.path.exists(SAVE_DIR): os.makedirs(SAVE_DIR)\n",
    "\n",
    "    # LOOP THROUGH PAGES (1 to 10)\n",
    "    for page in range(1, 11): \n",
    "        print(f\"\\n--- FETCHING PAGE {page} ---\")\n",
    "        search_url = f\"https://api.github.com/search/repositories?q=language:Java+created:>{CREATED_AFTER}+license:{LICENSE}&sort=stars&per_page=100&page={page}\"\n",
    "        \n",
    "        response = requests.get(search_url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Reached Rate Limit or Error: {response.text}\")\n",
    "            break\n",
    "\n",
    "        repos = response.json().get('items', [])\n",
    "        saved_files_overall = 0\n",
    "\n",
    "        if not repos: break\n",
    "\n",
    "        for repo in repos:\n",
    "            repo_name = repo['full_name']\n",
    "            # [Insert the same logic here to scan tree and download files...]\n",
    "            # (To save time, only scan the first 50 files per repo)\n",
    "            print(f\"Checking {repo_name}...\")\n",
    "                    \n",
    "            saved_files_repo = 0\n",
    "\n",
    "            # 2. Get the file tree (recursive)\n",
    "            tree_url = f\"https://api.github.com/repos/{repo_name}/git/trees/{repo['default_branch']}?recursive=1\"\n",
    "            tree_res = requests.get(tree_url, headers=HEADERS).json()\n",
    "            \n",
    "            if 'tree' not in tree_res: continue\n",
    "\n",
    "            for file in tree_res['tree']:\n",
    "                if saved_files_overall >= 5:\n",
    "                    break\n",
    "\n",
    "                if file['path'].endswith('.java'):\n",
    "                    # 3. Get file content\n",
    "                    file_url = file['url']\n",
    "                    blob = requests.get(file_url, headers=HEADERS).json()\n",
    "                    \n",
    "                    if 'content' not in blob: continue\n",
    "                    \n",
    "                    # Decode from Base64\n",
    "                    try:\n",
    "                        raw_content = base64.b64decode(blob['content']).decode('utf-8')\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "                    # 4. Apply filters\n",
    "                    if filter_java_content(raw_content):\n",
    "                        filename = f\"{repo_name.replace('/', '_')}_{file['path'].split('/')[-1]}\"\n",
    "                        with open(os.path.join(SAVE_DIR, filename), \"w\", encoding=\"utf-8\") as f:\n",
    "                            f.write(raw_content)\n",
    "                        print(f\"  [SAVED] {file['path']} ({len(raw_content.splitlines())} LOC)\")\n",
    "                        saved_files_overall += 1\n",
    "                        saved_files_repo += 1\n",
    "                        \n",
    "            if saved_files_overall >= 100:\n",
    "                break\n",
    "\n",
    "            time.sleep(1) # Be kind to the API\n",
    "\n",
    "        # Be careful with the Search Rate Limit (30 requests/min for authenticated users)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1632dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_bulk_java()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
